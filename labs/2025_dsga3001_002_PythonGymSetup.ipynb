{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a59b8dd",
   "metadata": {},
   "source": [
    "### CDS NYU\n",
    "### DS-GA 3001 | Reinforcement Learning\n",
    "### Python and Gym Setup\n",
    "### January 23, 2025\n",
    "\n",
    "\n",
    "# Installing OpenAI Gym (Gymnasium) in Python\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3143891",
   "metadata": {},
   "source": [
    "## Professor\n",
    "Jeremy Curuksu, PhD -- jeremy.cur@nyu.edu\n",
    "\n",
    "## Section Leaders\n",
    "Akshitha Kumbam – ak11071@nyu.edu\n",
    "\n",
    "Kushagra Khatwani – kk5395@nyu.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1147d",
   "metadata": {},
   "source": [
    "## Goal of Today's Lab \n",
    "\n",
    "In this Intro Lab, the goal is just to set up a Python environment with the Gym library and some of Gym dependencies to build RL solutions, such as the Arcade Learning Environment, Tensorflow, Keras-RL, etc.\n",
    "\n",
    "We will build a first \"RL environment\" in Gym just make sure Gym is installed successfully. Next week, we will dive deeper into Gym environments to understand the key components involved when working with Gym. For today, let's just focus on installing libraries needed for this course.\n",
    "\n",
    "You have two options to execute the code provided in the lab material: either locally on your laptop, or in the cloud on Google Colab. The instructional team will ensure all notebooks provided in the labs run on Google Colab. We will also provide some assistance (during lab sessions and office hours) to set up your local environment on your laptop. But we can't guarantee to solve all possible issues you may meet when installing libraries on your laptops and running the code in your local environment. The ability to run code locally has its advantages, such as the interactive gameplay (i.e., interact with RL environments and play RL games manually using your keyboard) which we will introduce as optional exercise in some of the labs and is only possible with a local setup. So we recommend you try to setup your system locally first, and use Google Colab only if you meet persistent issues locally. Debugging dependency issues is an important skill to have as a data scientist and ML practitioner, so please give it a try to install all libraries on your laptop!\n",
    "\n",
    "## Resources\n",
    "\n",
    "* Gym: https://www.gymlibrary.dev/ and its wiki https://github.com/openai/gym/wiki\n",
    "* The original paper from OpenAI when Gym was released in 2016: https://arxiv.org/pdf/1606.01540.pdf\n",
    "* In late 2022, Gym was moved to a new platform called Gymnasium, which is now **the only maintained version of Gym**: https://gymnasium.farama.org/\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f95e3",
   "metadata": {},
   "source": [
    "# 1. Install libraries to set up RL environments in Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60d9b0",
   "metadata": {},
   "source": [
    "#### At the minimum, you need to create a virtual environment with Python and OpenAI Gym installed:\n",
    "\n",
    "`conda create --name py39 python=3.9` \n",
    "\n",
    "`pip install gym`\n",
    "\n",
    "`pip install gymnasium`\n",
    "\n",
    "To add the virtual env as kernel in Jupyter Notebook:\n",
    "\n",
    "`conda activate py39`\n",
    "\n",
    "`pip install ipykernel`\n",
    "\n",
    "`python -m ipykernel install --user --name=py39`\n",
    "\n",
    "\n",
    "#### Other libraries will soon be needed as the course progresses:\n",
    "\n",
    "\n",
    "* Install **extended Gym packages** (e.g., Atari games, etc): `pip install gym-all` or `conda install -c conda-forge gym-all` and `pip install 'gym[atari,accept-rom-license]'`\n",
    "\n",
    "\n",
    "* Install the **Arcade Learning Environment**: `pip install ale-py`\n",
    "\n",
    "\n",
    "* Install **box2d**: \n",
    "`pip install gym-box2d` or `conda install -c conda-forge gym-box2d`\n",
    "\n",
    "\n",
    "* Install **pygame**: `pip install pygame` \n",
    "\n",
    "\n",
    "* Install **tensorflow**:\n",
    "`pip install tensorflow` or `conda install -c conda-forge tensorflow`\n",
    "\n",
    "\n",
    "* Install **keras-rl2**:\n",
    "`pip install keras-rl2`\n",
    "\n",
    "There will be a few others packages that need be installed during the semester, and some of them may be specific to your particular laptop setup and operating system. Feel free to seek guidance from Akshitha or Kushagra during office hours.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74106f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[accept-rom-license,atari] in /Users/alexman/Desktop/NYU_Materials/DS-GA-3001-Reinforcement/.venv/lib/python3.9/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/alexman/Desktop/NYU_Materials/DS-GA-3001-Reinforcement/.venv/lib/python3.9/site-packages (from gym[accept-rom-license,atari]) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/alexman/Desktop/NYU_Materials/DS-GA-3001-Reinforcement/.venv/lib/python3.9/site-packages (from gym[accept-rom-license,atari]) (3.1.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.8.0 in /Users/alexman/Desktop/NYU_Materials/DS-GA-3001-Reinforcement/.venv/lib/python3.9/site-packages (from gym[accept-rom-license,atari]) (8.6.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /Users/alexman/Desktop/NYU_Materials/DS-GA-3001-Reinforcement/.venv/lib/python3.9/site-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
      "Collecting ale-py~=0.8.0 (from gym[accept-rom-license,atari])\n",
      "  Downloading ale_py-0.8.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
      "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting importlib-resources (from ale-py~=0.8.0->gym[accept-rom-license,atari])\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/alexman/Desktop/NYU_Materials/DS-GA-3001-Reinforcement/.venv/lib/python3.9/site-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (4.12.2)\n",
      "Collecting click (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting requests (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
      "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: zipp>=3.20 in /Users/alexman/Desktop/NYU_Materials/DS-GA-3001-Reinforcement/.venv/lib/python3.9/site-packages (from importlib_metadata>=4.8.0->gym[accept-rom-license,atari]) (3.21.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
      "  Using cached charset_normalizer-3.4.1-cp39-cp39-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading ale_py-0.8.1-cp39-cp39-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp39-cp39-macosx_10_9_universal2.whl (197 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446667 sha256=8b71dc46e4a817546538cb029509d09d796badbee898360e3d82404638497f42\n",
      "  Stored in directory: /Users/alexman/Library/Caches/pip/wheels/b1/1f/f7/2da07cf4f81ea264bdaf043028749d88fe0c2227134a22cf80\n",
      "Successfully built AutoROM.accept-rom-license\n",
      "Installing collected packages: urllib3, tqdm, importlib-resources, idna, click, charset-normalizer, certifi, requests, ale-py, AutoROM.accept-rom-license, autorom\n",
      "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 certifi-2024.12.14 charset-normalizer-3.4.1 click-8.1.8 idna-3.10 importlib-resources-6.5.2 requests-2.32.3 tqdm-4.67.1 urllib3-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install 'gym[atari,accept-rom-license]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c7c0e",
   "metadata": {},
   "source": [
    "# 2. Build an Atari RL simulation environment in Gym\n",
    "\n",
    "In this section, we will build a first Reinforcement Learning environment in Gym just for you to confirm the key libraries have been successfully installed on your laptop. \n",
    "\n",
    "Next week, we will dive deeper into Gym environments to understand the key components involved when working with Gym. \n",
    "\n",
    "For today, let's just confirm Gym has been installed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3706019",
   "metadata": {},
   "source": [
    "Gym is a toolkit for developing and comparing reinforcement learning (RL) algorithms. It offers pre-built, baseline RL environments within which a developer can build and test RL algorithms. \n",
    "\n",
    "**At the most fundamental level, using the Gym library means 1) selecting an environment, and 2) interacting with it:**\n",
    "\n",
    "1. **Gym offers many different environments to select, from classic control use cases (Pendulum, Cart-Pole, Blackjack, etc) to video games (Atari) and simulated robotics (MuJoCo)**. These use cases were selected by OpenAI in 2016 to represent problems that are tractable using existing (21st century) AI technologies, yet complex enough to showcase the need for human-like intelligence.\n",
    "\n",
    "\n",
    "2. **Gym offers Python functions to interact with the created environment**. Most important ones are:\n",
    "    * `reset()`: Resets the state of the environment to the initial state (i.e., it restarts the game)\n",
    "    * `step(action)`:  Step forward by performing an action on the environment and returning the resulting state and reward after taking that action, a flag indicating if the game is over or not, and some metadata information\n",
    "    \n",
    "The `reset` function returns one value, which is a starting state/observation. \n",
    "\n",
    "The `step` function returns four values, which we will call the ``next_state``, ``reward``,  ``done``, ``truncated`` and ``info`` variables.\n",
    "\n",
    "-  ``next_state``: This is the observation that the agent will receive\n",
    "   after taking the action.\n",
    "-  ``reward``: This is the reward that the agent will receive after\n",
    "   taking the action.\n",
    "-  ``done``: This is a boolean variable that indicates whether or\n",
    "   not the environment has terminated.\n",
    "-  ``truncated``: This is a boolean variable that indicates whether timelimit is over or agent went physically out of bounds for the environment.\n",
    "-  ``info``: This is a dictionary that might contain additional\n",
    "   information about the environment.\n",
    "\n",
    "In the Atari environments the ``info`` dictionary has a ``ale.lives`` key that tells us how many lives the\n",
    "agent has left. If the agent has 0 lives, then the episode is over.\n",
    "\n",
    "\n",
    "### Here are the most basic Python commands to implement a Gym environment:\n",
    "\n",
    "A concise doc for the Atari Breakout video game available in Gym can be found here: https://gym.openai.com/envs/Breakout-v0/\n",
    "\n",
    "Note these basic commands are identical for all environments in Gym.\n",
    "\n",
    "\n",
    "**WARNING: Graphical rendering often crashes the Python kernel after completion => If this happens don't worry about it, just restart your kernel** (click on `Kernel`, then `Restart`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724aa579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v2', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'Reacher-v2', 'Reacher-v4', 'Pusher-v2', 'Pusher-v4', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'HumanoidStandup-v2', 'HumanoidStandup-v4'])\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print(gym.envs.registry.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8787c9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexman/Desktop/NYU_Materials/DS-GA-3001-Reinforcement/.venv/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Breakout-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m):\n\u001b[1;32m      5\u001b[0m     action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()  \u001b[39m# this is where an actor (RL agent) would be inserted\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     observation, reward, done, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m      8\u001b[0m         observation \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/Desktop/NYU_Materials/DS-GA-3001-Reinforcement/.venv/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Desktop/NYU_Materials/DS-GA-3001-Reinforcement/.venv/lib/python3.9/site-packages/gym/wrappers/env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/Desktop/NYU_Materials/DS-GA-3001-Reinforcement/.venv/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    230\u001b[0m obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m result\n\u001b[1;32m    232\u001b[0m \u001b[39m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(terminated, (\u001b[39mbool\u001b[39m, np\u001b[39m.\u001b[39;49mbool8)):\n\u001b[1;32m    234\u001b[0m     logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    235\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(terminated)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(truncated, (\u001b[39mbool\u001b[39m, np\u001b[39m.\u001b[39mbool8)):\n",
      "File \u001b[0;32m~/Desktop/NYU_Materials/DS-GA-3001-Reinforcement/.venv/lib/python3.9/site-packages/numpy/__init__.py:410\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchar\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mchar\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[39mreturn\u001b[39;00m char\u001b[39m.\u001b[39mchararray\n\u001b[0;32m--> 410\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Breakout-v0\", render_mode=\"human\") # Exact name/version of environments can be found in Gym's doc\n",
    "observation = env.reset()\n",
    "for _ in range(500):\n",
    "    action = env.action_space.sample()  # this is where an actor (RL agent) would be inserted\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e21b60",
   "metadata": {},
   "source": [
    "**Warning:** The `render_mode=\"human\"` argument renders the environment graphically in a separate window. It is optional, and not always recommended. It is not a good idea to use it when *training* an agent because rendering slows down training a lot. But when looking at an environment for the first time, or when training is complete, it can of course be useful to graphically vizualize how the agent behaves in the environment. But here is the warning: in Jupyter Notebook the Gym's graphical rendering works well but is likely to crash your kernel (or freeze it) once the simulation is complete. So as a habit, be ready to click on `Interrupt` and `Restart` in the Kernel tab of Jupyter Notebook after you run a simulation rendered graphically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2c6f4",
   "metadata": {},
   "source": [
    "# Thank you everyone, and welcome again to DS-GA 3001 : Reinforcement Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
