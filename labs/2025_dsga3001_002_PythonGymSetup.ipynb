{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a59b8dd",
   "metadata": {},
   "source": [
    "### CDS NYU\n",
    "### DS-GA 3001 | Reinforcement Learning\n",
    "### Python and Gym Setup\n",
    "### January 23, 2025\n",
    "\n",
    "\n",
    "# Installing OpenAI Gym (Gymnasium) in Python\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3143891",
   "metadata": {},
   "source": [
    "## Professor\n",
    "Jeremy Curuksu, PhD -- jeremy.cur@nyu.edu\n",
    "\n",
    "## Section Leaders\n",
    "Akshitha Kumbam – ak11071@nyu.edu\n",
    "\n",
    "Kushagra Khatwani – kk5395@nyu.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1147d",
   "metadata": {},
   "source": [
    "## Goal of Today's Lab \n",
    "\n",
    "In this Intro Lab, the goal is just to set up a Python environment with the Gym library and some of Gym dependencies to build RL solutions, such as the Arcade Learning Environment, Tensorflow, Keras-RL, etc.\n",
    "\n",
    "We will build a first \"RL environment\" in Gym just make sure Gym is installed successfully. Next week, we will dive deeper into Gym environments to understand the key components involved when working with Gym. For today, let's just focus on installing libraries needed for this course.\n",
    "\n",
    "You have two options to execute the code provided in the lab material: either locally on your laptop, or in the cloud on Google Colab. The instructional team will ensure all notebooks provided in the labs run on Google Colab. We will also provide some assistance (during lab sessions and office hours) to set up your local environment on your laptop. But we can't guarantee to solve all possible issues you may meet when installing libraries on your laptops and running the code in your local environment. The ability to run code locally has its advantages, such as the interactive gameplay (i.e., interact with RL environments and play RL games manually using your keyboard) which we will introduce as optional exercise in some of the labs and is only possible with a local setup. So we recommend you try to setup your system locally first, and use Google Colab only if you meet persistent issues locally. Debugging dependency issues is an important skill to have as a data scientist and ML practitioner, so please give it a try to install all libraries on your laptop!\n",
    "\n",
    "## Resources\n",
    "\n",
    "* Gym: https://www.gymlibrary.dev/ and its wiki https://github.com/openai/gym/wiki\n",
    "* The original paper from OpenAI when Gym was released in 2016: https://arxiv.org/pdf/1606.01540.pdf\n",
    "* In late 2022, Gym was moved to a new platform called Gymnasium, which is now **the only maintained version of Gym**: https://gymnasium.farama.org/\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f95e3",
   "metadata": {},
   "source": [
    "# 1. Install libraries to set up RL environments in Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60d9b0",
   "metadata": {},
   "source": [
    "#### At the minimum, you need to create a virtual environment with Python and OpenAI Gym installed:\n",
    "\n",
    "`conda create --name py39 python=3.9` \n",
    "\n",
    "`pip install gym`\n",
    "\n",
    "`pip install gymnasium`\n",
    "\n",
    "To add the virtual env as kernel in Jupyter Notebook:\n",
    "\n",
    "`conda activate py39`\n",
    "\n",
    "`pip install ipykernel`\n",
    "\n",
    "`python -m ipykernel install --user --name=py39`\n",
    "\n",
    "\n",
    "#### Other libraries will soon be needed as the course progresses:\n",
    "\n",
    "\n",
    "* Install **extended Gym packages** (e.g., Atari games, etc): `pip install gym-all` or `conda install -c conda-forge gym-all` and `pip install 'gym[atari,accept-rom-license]'`\n",
    "\n",
    "\n",
    "* Install the **Arcade Learning Environment**: `pip install ale-py`\n",
    "\n",
    "\n",
    "* Install **box2d**: \n",
    "`pip install gym-box2d` or `conda install -c conda-forge gym-box2d`\n",
    "\n",
    "\n",
    "* Install **pygame**: `pip install pygame` \n",
    "\n",
    "\n",
    "* Install **tensorflow**:\n",
    "`pip install tensorflow` or `conda install -c conda-forge tensorflow`\n",
    "\n",
    "\n",
    "* Install **keras-rl2**:\n",
    "`pip install keras-rl2`\n",
    "\n",
    "There will be a few others packages that need be installed during the semester, and some of them may be specific to your particular laptop setup and operating system. Feel free to seek guidance from Akshitha or Kushagra during office hours.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c7c0e",
   "metadata": {},
   "source": [
    "# 2. Build an Atari RL simulation environment in Gym\n",
    "\n",
    "In this section, we will build a first Reinforcement Learning environment in Gym just for you to confirm the key libraries have been successfully installed on your laptop. \n",
    "\n",
    "Next week, we will dive deeper into Gym environments to understand the key components involved when working with Gym. \n",
    "\n",
    "For today, let's just confirm Gym has been installed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3706019",
   "metadata": {},
   "source": [
    "Gym is a toolkit for developing and comparing reinforcement learning (RL) algorithms. It offers pre-built, baseline RL environments within which a developer can build and test RL algorithms. \n",
    "\n",
    "**At the most fundamental level, using the Gym library means 1) selecting an environment, and 2) interacting with it:**\n",
    "\n",
    "1. **Gym offers many different environments to select, from classic control use cases (Pendulum, Cart-Pole, Blackjack, etc) to video games (Atari) and simulated robotics (MuJoCo)**. These use cases were selected by OpenAI in 2016 to represent problems that are tractable using existing (21st century) AI technologies, yet complex enough to showcase the need for human-like intelligence.\n",
    "\n",
    "\n",
    "2. **Gym offers Python functions to interact with the created environment**. Most important ones are:\n",
    "    * `reset()`: Resets the state of the environment to the initial state (i.e., it restarts the game)\n",
    "    * `step(action)`:  Step forward by performing an action on the environment and returning the resulting state and reward after taking that action, a flag indicating if the game is over or not, and some metadata information\n",
    "    \n",
    "The `reset` function returns one value, which is a starting state/observation. \n",
    "\n",
    "The `step` function returns four values, which we will call the ``next_state``, ``reward``,  ``done``, ``truncated`` and ``info`` variables.\n",
    "\n",
    "-  ``next_state``: This is the observation that the agent will receive\n",
    "   after taking the action.\n",
    "-  ``reward``: This is the reward that the agent will receive after\n",
    "   taking the action.\n",
    "-  ``done``: This is a boolean variable that indicates whether or\n",
    "   not the environment has terminated.\n",
    "-  ``truncated``: This is a boolean variable that indicates whether timelimit is over or agent went physically out of bounds for the environment.\n",
    "-  ``info``: This is a dictionary that might contain additional\n",
    "   information about the environment.\n",
    "\n",
    "In the Atari environments the ``info`` dictionary has a ``ale.lives`` key that tells us how many lives the\n",
    "agent has left. If the agent has 0 lives, then the episode is over.\n",
    "\n",
    "\n",
    "### Here are the most basic Python commands to implement a Gym environment:\n",
    "\n",
    "A concise doc for the Atari Breakout video game available in Gym can be found here: https://gym.openai.com/envs/Breakout-v0/\n",
    "\n",
    "Note these basic commands are identical for all environments in Gym.\n",
    "\n",
    "\n",
    "**WARNING: Graphical rendering often crashes the Python kernel after completion => If this happens don't worry about it, just restart your kernel** (click on `Kernel`, then `Restart`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8787c9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Breakout-v0\", render_mode=\"human\") # Exact name/version of environments can be found in Gym's doc\n",
    "observation = env.reset()\n",
    "for _ in range(500):\n",
    "    action = env.action_space.sample()  # this is where an actor (RL agent) would be inserted\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e21b60",
   "metadata": {},
   "source": [
    "**Warning:** The `render_mode=\"human\"` argument renders the environment graphically in a separate window. It is optional, and not always recommended. It is not a good idea to use it when *training* an agent because rendering slows down training a lot. But when looking at an environment for the first time, or when training is complete, it can of course be useful to graphically vizualize how the agent behaves in the environment. But here is the warning: in Jupyter Notebook the Gym's graphical rendering works well but is likely to crash your kernel (or freeze it) once the simulation is complete. So as a habit, be ready to click on `Interrupt` and `Restart` in the Kernel tab of Jupyter Notebook after you run a simulation rendered graphically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2c6f4",
   "metadata": {},
   "source": [
    "# Thank you everyone, and welcome again to DS-GA 3001 : Reinforcement Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
