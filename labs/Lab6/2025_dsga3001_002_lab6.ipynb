{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDS NYU\n",
    "### DS-GA 3001 | Reinforcement Learning\n",
    "### Lab 06\n",
    "### March 06, 2025\n",
    "\n",
    "\n",
    "# Implement RL algorithms with Keras-RL or Stable-Baseline\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Leader\n",
    "\n",
    "\n",
    "Akshitha Kumbam – ak11071@nyu.edu\n",
    "\n",
    "Kushagra Khatwani – kk5395@nyu.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of Today's Lab \n",
    "\n",
    "In this Lab, we will implement RL algorithms by building on existing RL algorithm libraries, so we don't have to implement RL agents from scratch as we did in the past few weeks. By doing so, we get less control on the details of the implementation, but it is much faster to implement (**we will cover 4 case studies today:** `CartPole`, `SpaceInvaders`, `CarRacing`, and `StockTrading`), and components available in public libraries tend to be high quality and efficient. \n",
    "\n",
    "Let us start with the DQN algorithm. We can use external open-source Python packages which implement each of the key DQN methods (e,g., experience replay method, action-selection method, etc). These methods are developed, maintained, and optimized for robustness to different scenarios, and for overall performance.\n",
    "\n",
    "Using these packages instead of implementing each DQN component from scratch is generally faster, leads to a more reliable/efficient program, yet still gives you a lot of control on the details of the implementation and hyperparameters.\n",
    "\n",
    "We will focus on DQN in the first part of the lab, but these packages also offer methods for most commonly used RL algorithms such as A3C, PPO, etc, so we will start looking at these too.\n",
    "\n",
    "## Resources\n",
    "\n",
    "* https://gymnasium.farama.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implement DQN with Keras-RL Methods\n",
    "\n",
    "Keras-RL is an older reinforcement learning library that does not support the latest versions of TensorFlow and Gym. To ensure compatibility, we need to downgrade both TensorFlow and Gym to specific versions that work with Keras-RL.\n",
    "\n",
    "### Install Required Dependencies\n",
    "\n",
    "To install the required versions, run the following commands:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow==2.15\n",
    "pip install gym==0.15.4\n",
    "pip install pyglet==1.5.11\n",
    "```\n",
    "\n",
    "**Note:** We use `gym` instead of `gymnasium` because Keras-RL was originally built for `gym` and does not support `gymnasium`. Using these specific versions ensures that we can successfully implement and train a Deep Q-Network (DQN) using Keras-RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Solve *Cart Pole*  with DQN from Keras-rl\n",
    "\n",
    "Most of this case study will be the same as in previous lab, but we build some of the key components of the DQN agent using components available in the Keral-RL library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import gym\n",
    "from pyglet.window import key \n",
    "from tensorflow.keras.models import Sequential  \n",
    "from tensorflow.keras.layers import Dense  \n",
    "from tensorflow.keras.layers import Activation \n",
    "from tensorflow.keras.layers import Flatten  \n",
    "from tensorflow.keras.optimizers.legacy import Adam  # Adam optimizer\n",
    "\n",
    "\n",
    "# Import DQN methods from the keras-rl2 library (keras-rl is tagged \"rl\" in Python)\n",
    "# Quick fix if python cannot import name '__version__' from 'tensorflow.keras'\n",
    "import tensorflow as tf\n",
    "from keras import __version__\n",
    "tf.keras.__version__ = __version__\n",
    "\n",
    "from rl.agents.dqn import DQNAgent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the `CartPole` Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/56904270/difference-between-openai-gym-environments-cartpole-v0-and-cartpole-v1\n",
    "env_name = ENV_NAME = 'CartPole-v1'\n",
    "env = gym.make(env_name)  \n",
    "\n",
    "# Same as last week:\n",
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape[0]\n",
    "print(f\"There are {num_actions} possible actions and {num_observations} observations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute random actions just to get familiar with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the CartPole Gym environment with graphical rendering to vizualize the environment\n",
    "env_test = gym.make(\"CartPole-v1\")  # [Jeremy] was v1 last week  \n",
    "# Set to initial state\n",
    "env_test.reset()\n",
    "  \n",
    "\n",
    "# Loop over 200 steps\n",
    "for _ in range(200):\n",
    "    env_test.render()                                                 # Render on the screen\n",
    "    action = env_test.action_space.sample()                           # Choose a random action\n",
    "    new_state, reward, done, info = env_test.step(action)  # Carry out the action\n",
    "    time.sleep(0.03)\n",
    "    if done:\n",
    "         env_test.reset()\n",
    "            \n",
    "env_test.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement an Artificial Neural Network\n",
    "To build our network, we first need to find out how many actions and observation our environment has.\n",
    "We can either get those information from the source code (https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) or via the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to in the previous lab, we build a simple ANN with 2 hidden layers and 16 and 32 neurons each followed by relu activation. The output layer has 2 nodes, one for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# https://keras.io/api/layers/reshaping_layers/flatten/\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the DQN framework with Keras-RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DQN agent `DQNagent` from Keras-RL takes the following parameters:\n",
    "\n",
    "1. `model` = The ANN\n",
    "\n",
    "\n",
    "2. `nb_actions` = The number of actions (2 in this case)\n",
    "\n",
    "\n",
    "3. `memory` = The action replay memory. Ny far the most common choice is `SequentialMemory()` \n",
    "\n",
    "\n",
    "4. `nb_steps_warmup` = Number of iterations used to fill the memory prior starting to update the ANN parameters\n",
    "\n",
    "\n",
    "5. `target_model_update` = How often (in number of steps) to update the target model\n",
    "\n",
    "\n",
    "6. `policy` = You can choose between `LinearAnnealedPolicy()`, `SoftmaxPolicy()`, `EpsGreedyQPolicy()`, `GreedyQPolicy()`, `GreedyQPolicy()`, `MaxBoltzmannQPolicy()` and `BoltzmannGumbelQPolicy()`. We will use the `LinearAnnealedPolicy` policy, but feel free to try them out and inspect which works best here\n",
    "\n",
    "\n",
    "There are some more parameters you can pass to the DQN Agent, feel free to explore them on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's initialize a circular buffer with a limit of 20000 and window length of 1 (window length describes the number of steps stored to define a state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory  # Sequential Memory for storing observations (optimized circular buffer)\n",
    "\n",
    "memory = SequentialMemory(limit=20000, window_length=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the Action Selection Policy: <br />\n",
    "We use *LinearAnnealedPolicy* in order to perform the epsilon greedy strategy with decaying epsilon. <br />\n",
    "*LinearAnnealedPolicy* accepts an action selection policy, its maximal and minimal values and a step number in order to create a dynamical policy. <br/>\n",
    "The smallest value epsilon can reach during training is 0.1.<br />\n",
    "For testing/evaluation of the trained agent, let's set epsilon to 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearAnnealedPolicy allows to decay the epsilon for the epsilon greedy strategy\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps',\n",
    "                              value_max=1.,\n",
    "                              value_min=.1,\n",
    "                              value_test=.05,\n",
    "                              nb_steps=20000) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the DQN Agent based on the defined model (**model**), the possible actions (**num_actions**) (left and right in this case), the circular buffer (**memory**), the warmup phase (**10**), how often the target model gets updated (**100**) and the policy (**policy**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model = model, nb_actions = num_actions, memory = memory, nb_steps_warmup = 10,\n",
    "               target_model_update = 100, policy = policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compile the DQN with the Adam optimizer and a learning rate of 0.001.<br />\n",
    "We log the Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use learning_rate instead of lr if you get warning\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the training for 20000 steps. You can change visualize=True if you want to watch your model learning.\n",
    "Keep in mind that this increases the running time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps=20000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After just 1-2 minutes of training (for reference, it takes < 1min on a Macbook Air with M2), we achieve some great results already. It was taking at least 15 minutes of training to reach a similar level of performance with the custom DQN implemented from scratch in the previous lab.\n",
    "\n",
    "The reason for this is that keras-rl has implemented many optimization strategies (e.g., the optimized replay buffer) which lead to a significantly faster convergence than the DQN we implemented by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done, we can save the final weights.\n",
    "dqn.save_weights(f'dqn_{env_name}_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploit learned Q values in test simulations\n",
    "\n",
    "The Keras-RL agents also offer methods to perform tests in Gym, with some parameters e.g. to decide whether to render the simulation graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, we accomplished a better agent (trained more efficiently) with much less code than in the previous lab, thanks to Keras-RL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "# 1.2 Solve *Space Invaders*  with Convolutional DQN from Keras-rl\n",
    "Also, install :\n",
    "```bash\n",
    "pip install \"gym[atari,accept-rom-license]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# Quick fix if python cannot import name '__version__' from 'tensorflow.keras'\n",
    "import tensorflow as tf\n",
    "from keras import __version__\n",
    "tf.keras.__version__ = __version__\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make('SpaceInvaders-v0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute random actions just to get familiar with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        state, reward, done, info = env.step(env.action_space.sample())\n",
    "        print(state.shape)\n",
    "        score += reward\n",
    "    print('Episode: {}\\nScore: {}'.format(episode, score))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3, height, width, channels)))\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "print(height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model # [Jeremy] why do we need this? TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the DQN framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=2000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=False, dueling_type='avg',\n",
    "                  nb_actions=actions, nb_steps_warmup=1000)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps = 1000, visualize = False, verbose = 1) # Train for 1000 steps in class, try 40000 at home :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dqn.save_weights('SpaceInvaderTrainedModel/dqn.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploit learned Q values in test simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dqn.load_weights('SpaceInvaderTrainedModel/dqn.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0') \n",
    "\n",
    "scores = dqn.test(env, nb_episodes = 10, visualize = True)  # Would need play with versioning to use the vizualize parameter (currently fixed with line above)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement DQN with Stable-baselines algorithms\n",
    "\n",
    "Instead of implementing DQN from scratch (previous lab), or its individual components from external packages as seen in the section above, yet other packages exist which offer a \"complete\" implementation of the most popular RL algorithms (DQN, A3C, PPO, DDPG, etc).\n",
    "\n",
    "Many such packages exist. The most popular ones include OpenAI `Stable-baselines`, DeepMind `Acme`, AWS `SageMaker RL`, Meta `AI ReAgent`, Ray `RLlib`, Intel `AI Coach`. \n",
    "\n",
    "Today we will focus exclusively on `Stable-baselines` because it was initially designed by OpenAI in tandem with `Gym` environments. In the final labs of the semester will introduce several other frameworks.\n",
    "\n",
    "\n",
    "These implementations of RL algorithms have been optimized for ease-of-use, robustness to different scenarios, and overall performance. Their drawback is you get less control on details of the implementation and hyperparameters, compared to using packages such as `Keras-RL` which only implement key *components* of RL algorithms.\n",
    "\n",
    "Please install the following :\n",
    "```bash\n",
    "pip install stable-baselines3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Play 'Car Racing' with Convolutional PPO from Stable-baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "#Quick fix for M1 architecture (M2/M3 might also need this).\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from stable_baselines3 import PPO \n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the `CarRacing` Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v3')\n",
    "env.observation_space.sample().shape\n",
    "env.action_space.sample()\n",
    "#env.reset()\n",
    "#env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute random actions just to get familiar with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_test = gym.make('CarRacing-v3', render_mode = 'human')\n",
    "episodes = 5\n",
    "for episode in range(episodes):\n",
    "    state, info = env_test.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env_test.render()\n",
    "        action = env_test.action_space.sample()\n",
    "        n_state, reward, done, trunc, info = env_test.step(action)\n",
    "        score+=reward\n",
    "    print(\"Episode: {} Score: {}\".format(episode, score))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b279691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "# env = make_vec_env(lambda: gym.make('CarRacing-v3'), n_envs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the `PPO` algorithm from `stable-baselines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploit the trained agent in test simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the policy\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Trade a S&P 500 stock index with DQN from Stable-baselines\n",
    "Please install the following:\n",
    "```bash\n",
    "pip install gym_anytrading\n",
    "pip install yfinance  \n",
    "pip install pandas_datareader\n",
    "pip install TA\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "#Quick fix for M1 architecture (M2/M3 might also need this).\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv \n",
    "from stable_baselines3 import A2C \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For this case study please install the following libraries: gym_anytrading, yfinance, pandas_datareader, TA\n",
    "import gym_anytrading\n",
    "from gym_anytrading.envs import StocksEnv\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "# yf.pdr_override() # bug fix, for details see https://stackoverflow.com/questions/74862453/why-am-i-getting-a-typeerror-string-indices-must-be-integer-message-when-tryi\n",
    "from ta import add_all_ta_features # Method from TA (Technical Analysis) library to engineer financial indicators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the S&P 500 stock index trading environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read some daily time series stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = yf.download('SPY', start='2021-01-01', end='2023-01-01', multi_level_index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index(\"Date\", inplace = True)\n",
    "df = df.reset_index()\n",
    "df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Gym trading environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('stocks-v0', df=df, frame_bound=(5, 400), window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96fd666",
   "metadata": {},
   "source": [
    "## Execute random actions just to get familiar with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "state, info = env.reset()\n",
    "\n",
    "# Run the environment with random actions (Buy or Sell)\n",
    "while True:  # Example: run for 100 steps\n",
    "    action = env.action_space.sample()  # Sample random action (0 or 1)\n",
    "    next_state, reward, done, trunc, info = env.step(action)  # Take the action\n",
    "    \n",
    "    # Print the action and reward\n",
    "    print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "    \n",
    "    if done or trunc:\n",
    "        print(\"Episode finished. Info:\", info)\n",
    "        break\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.cla()\n",
    "env.unwrapped.render_all()  # Render the environment\n",
    "plt.show()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green dots represent buying while the red dots represent selling stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an `A2C` algorithm from `stable-baselines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('stocks-v0', df=df, frame_bound=(5, 400), window_size=5)\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploit the trained agent in test simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('stocks-v0', df=df, frame_bound=(400, 502), window_size=5)\n",
    "obs, info = env.reset()\n",
    "while True:\n",
    "    # obs = obs[np.newaxis, ...]\n",
    "    action, states = model.predict(obs)\n",
    "    obs, rewards, done,trunc,  info = env.step(action)\n",
    "    \n",
    "    if done or trunc:\n",
    "        print(info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.cla()\n",
    "env.unwrapped.render_all()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the green dots represent buying while the red dots represent selling stocks. If the agent is capable of making intelligent trading decisions, it will tend to buy (green dots) when the price is relatively low, and to sell (red dots) when the price is relatively high. \n",
    "\n",
    "As you can see, the agent needs fine tuning to perform well. So to close the lab, let us fine tune the agent by engineering features (\"financial indicators\") and adding these additional indicators to the definition of the RL state, so the agent has additional information every day to make trading decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineer features (financial indicators) for the agent to make smarter decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = yf.download('SPY', start='2021-01-01', end='2023-01-01', multi_level_index=False)\n",
    "# Engineer financial indicators using the method imported above from the \"TA\" library\n",
    "df2 = add_all_ta_features(data, open='Open', high='High', low='Low', close='Close', volume='Volume', fillna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_processed_data(env):\n",
    "    start = env.frame_bound[0] - env.window_size\n",
    "    end = env.frame_bound[1]\n",
    "    prices = env.df.loc[:, 'Low'].to_numpy()[start:end]\n",
    "    signal_features = env.df.loc[:, ['Close', 'Volume', 'momentum_rsi', 'volume_obv', 'trend_macd_diff']].to_numpy()[start:end]\n",
    "    return prices, signal_features\n",
    "\n",
    "class MyCustomEnv(StocksEnv):\n",
    "    _process_data = my_processed_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = MyCustomEnv(df=df2, window_size= 5, frame_bound=(5, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-train the `A2C` algorithm from `stable-baselines` with the new engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = A2C('MlpPolicy', env2, verbose=1)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploit the trained agent in test simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyCustomEnv(df=df2, window_size=5, frame_bound=(400, 502))\n",
    "obs, info = env.reset()\n",
    "\n",
    "while True:\n",
    "    # obs = obs[np.newaxis, ...]\n",
    "    action, states = model.predict(obs)\n",
    "    obs, rewards, done, trunc, info = env.step(action)\n",
    "    \n",
    "    if done or trunc:\n",
    "        print(info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.cla()\n",
    "env.render_all()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you now see more green dots (buy stocks) when the price is relatively low, and more red dots (sell stocks) when the price is relatively high!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: What other fine tuning can you try to improve the agent's trading performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Implement the `CartPole` and `SpaceInvaders` agents using the DQN algorithms from `stable-baselines`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Implement the `PPO` algorithm from `stable-baselines` to learn to play at the `FlappyBird` (lab 5) and `SpaceInvaders` games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you everyone!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_keras_rl",
   "language": "python",
   "name": "py39_keras_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
