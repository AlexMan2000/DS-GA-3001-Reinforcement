{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDS NYU\n",
    "### DS-GA 3001 | Reinforcement Learning\n",
    "### Lab 05\n",
    "### February 27, 2025\n",
    "\n",
    "\n",
    "# Deep Q-learning algorithm (from scratch...)\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Leader\n",
    "\n",
    "\n",
    "Akshitha Kumbam – ak11071@nyu.edu\n",
    "\n",
    "Kushagra Khatwani – kk5395@nyu.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of Today's Lab \n",
    "\n",
    "In this lab, we will create a Deep Reinforcement Learning method called Deep-Q-Network (DQN). We will again use a simple environment from OpenAI Gym, but you will showcase the enormous gain we get by switching from tabular Q-Learning to Deep Q Learning.\n",
    "\n",
    "## Resources\n",
    "\n",
    "* https://gymnasium.farama.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please activate a cloned version of your anaconda environment for this lab as some packages may require different versions of gym.\n",
    "\n",
    "### Do not forget to change kernel to the cloned env."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Solve *Cart Pole*  with Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CartPole` environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in “*Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem*”. A pendulum (\"pole\") is attached by an un-actuated joint to a cart, placed upright on the cart, and the cart can move along a frictionless track. \n",
    "\n",
    "The pendulum is placed upright on the cart and the goal is to balance the pole (i.e., keep it upright within -12 to +12 degrees) by applying forces in the left and right direction on the cart, using two possible actions: <br>\n",
    "`0`: Push cart to the left <br>\n",
    "`1`: Push cart to the right\n",
    "\n",
    "**Challenges**: The episode terminates if the cart x-position gets outside the range [-2.4, 2.4] and/or the pole angle gets outside the range [-12°, 12°]. What makes this problem non-trivial is that the velocity which is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it.\n",
    "\n",
    "\n",
    "Details can be found in the Gym/Gymnasium doc: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We will use tensorflow libraries to create a deep learning neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym  \n",
    "from tensorflow.keras.models import Sequential  # To compose multiple Layers\n",
    "from tensorflow.keras.layers import Dense       # Fully-Connected layer\n",
    "from tensorflow.keras.layers import Activation  # Activation functions\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import clone_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute random actions just to get familiar with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CartPole Gym environment with graphical rendering to vizualize the environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")   \n",
    "\n",
    "# Set to initial state\n",
    "env.reset()  \n",
    "\n",
    "# Loop over 200 steps\n",
    "for _ in range(200):\n",
    "    env.render()                                                 # Render on the screen\n",
    "    action = env.action_space.sample()                           # Choose a random action\n",
    "    new_state, reward, done, truncated, info = env.step(action)  # Carry out the action\n",
    "    if done or truncated:\n",
    "        env.reset()\n",
    "            \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement an Artificial Neural Network\n",
    "To build the Q-network (referred ot as *model* in the code), we first need to find out how many actions and observations our environment has.\n",
    "We can either get those information from the source code (https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) or via the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\") \n",
    "\n",
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape[0]  \n",
    "print(f\"There are {num_actions} possible actions and {num_observations} observations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our network needs to have an input dimension of 4 and an output dimension of 2.\n",
    "In between we are free to chose.\n",
    "\n",
    "Let's use a four layer architecture:\n",
    "\n",
    "1. The first layer has 16 neurons\n",
    "2. The second layer has 32 neurons\n",
    "4. The fourth layer (output layer) has 2 neurons\n",
    "\n",
    "This yields 690 parameters\n",
    "$$ \\text{4 observations} * 16 (\\text{neurons}) + 16 (\\text{bias}) + (16*32) + 32 + (32*2)+2 = 690$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(16, input_shape=(1, num_observations)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our model which takes an observation as input and outputs a value for each action.\n",
    "The higher the value, the more likely this value is a suitable action for the current observation.\n",
    "\n",
    "As stated in the lecture, Deep-Q-Learning works better when using a target network. So let's copy the above network to define a separate, target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"34.ckt\")\n",
    "target_model = clone_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up DQN hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "epsilon = 1.0\n",
    "EPSILON_REDUCE = 0.995  \n",
    "LEARNING_RATE = 0.001 \n",
    "GAMMA = 0.95\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the epsilon greedy action selection function once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(model, epsilon, observation):\n",
    "    obs=[]\n",
    "    if np.random.random() > epsilon:\n",
    "        #print(f\"*** Taking Greedy Action, observation shape 1: {observation.shape}\")\n",
    "        observation = observation.reshape([1, 1, 4]) \n",
    "        #print(f\"*** Taking Greedy Action, observation shape 2: {observation.shape}\")\n",
    "        prediction = model.predict(observation, verbose=0)  # Perform the prediction on the observation\n",
    "        action = np.argmax(prediction)           # Chose the action with the highest value\n",
    "    else:\n",
    "        #print(f\"*** Taking a random action\")\n",
    "        action = np.random.randint(0, env.action_space.n)  # Select random action with probability epsilon\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the lecture, we need a replay buffer.\n",
    "We can use the Python `deque` data structure for this. The *maxlen* argument specifies the number of elements the buffer can store before it starts overwriting elements at the beginning of the queue.\n",
    "\n",
    "The following cell shows an example using the deque function. You can see in the first example all values fit into the deque, so nothing is overwritten. \n",
    "\n",
    "In the second example, the deque is printed in each iteration. It can hold all values in the first five iterations but then needs to delete the oldest value in the deque to make room for the new value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPTIONAL CODE | EXAMPLE OF DEQUE]\n",
    "# In this example all values fit into the deque, no overwriting\n",
    "deque_1 = deque(maxlen=5)\n",
    "for i in range(5):    \n",
    "    deque_1.append(i)\n",
    "print(deque_1)\n",
    "print(\"---------------------\")\n",
    "\n",
    "# In this example, qfter the first 5 values are stored, it needs to overwrite the oldest value to store the new one\n",
    "deque_2 = deque(maxlen=5)\n",
    "for i in range(10):  \n",
    "    deque_2.append(i)\n",
    "    print(deque_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we allow our replay buffer a maximum size of 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=20000)\n",
    "update_target_model = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the lecture, experience replaying is very useful to stabilize training in Deep Q-Learning. <br />\n",
    "The following cell implements one version of the experience replay algorithm. <br />\n",
    "It uses the zip statement paired with the * operator (unpacking argument lists) to create mini-batches from the samples of experience accumulated.<br />\n",
    "The zip statement returns all corresponding pairs from each entry. <br />\n",
    "It might look confusing but the following example should clarify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuple = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n",
    "zipped_list = list(zip(*test_tuple))\n",
    "a, b, c = zipped_list\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(replay_buffer, batch_size, model, target_model):\n",
    "    \n",
    "    # As long as the buffer has not enough elements we do nothing\n",
    "    if len(replay_buffer) < batch_size: \n",
    "        return\n",
    "    \n",
    "    # Take a random sample from the buffer with size batch_size\n",
    "    samples = random.sample(replay_buffer, batch_size)  \n",
    "    \n",
    "    # Initialize variable to store the targets predicted by the target network for training\n",
    "    target_batch = []  \n",
    "    \n",
    "    # Efficient way to handle the sample by using the zip functionality\n",
    "    zipped_samples = list(zip(*samples))  \n",
    "    states, actions, rewards, new_states, dones = zipped_samples  \n",
    "    \n",
    "    # Predict targets for all states from the sample\n",
    "    #print(f\"*** *** *** *** EXPERIENCE REPLAY, length states: {len(states)}\")\n",
    "    #print(f\"*** *** *** *** EXPERIENCE REPLAY, states: {np.array(states).shape}\")\n",
    "    #print(f\"*** *** *** *** EXPERIENCE REPLAY, states: {np.array(states[0]).shape}\")\n",
    "    targets = target_model.predict(np.array(states), verbose=0)\n",
    "    \n",
    "    # Predict Q-Values for all new states from the sample\n",
    "    q_values = model.predict(np.array(new_states), verbose=0)  \n",
    "    \n",
    "    # Now we loop over all predicted values to compute the actual targets\n",
    "    for i in range(batch_size):  \n",
    "        \n",
    "        # Take the maximum Q-Value for each sample\n",
    "        q_value = max(q_values[i][0])  \n",
    "        \n",
    "        # Store the ith target in order to update it according to the formula\n",
    "        target = targets[i].copy()  \n",
    "        if dones[i]:\n",
    "            target[0][actions[i]] = rewards[i]\n",
    "        else:\n",
    "            target[0][actions[i]] = rewards[i] + q_value * GAMMA\n",
    "        target_batch.append(target)\n",
    "\n",
    "    # Fit the model based on the states and the updated targets for 1 epoch\n",
    "    model.fit(np.array(states), np.array(target_batch), epochs=1, verbose=0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to update the target network every once in a while. <br />\n",
    "Keras provides the *set_weights()* and *get_weights()* methods which can do the work for us, so we only need to check whether we hit an update epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_handler(epoch, update_target_model, model, target_model):\n",
    "    if epoch > 0 and epoch % update_target_model == 0:\n",
    "        target_model.set_weights(model.get_weights())\n",
    "        #print(f\"*** Debug: Updating model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=Adam(learning_rate=LEARNING_RATE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use provided chekpoints as starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_so_far = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    observation, info = env.reset()  # Get inital state\n",
    "    \n",
    "    # Keras expects the input to be of shape [1, X] thus we have to reshape\n",
    "    # [Jeremy] Original state is an array of shape (4): [Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity]\n",
    "    observation = observation.reshape([1, 4])  \n",
    "    #print(f\"*** Debug, observation shape: {observation.shape}\")\n",
    "    done = False  \n",
    "    \n",
    "    points = 0\n",
    "    while not done:  # As long current run is active\n",
    "        # Select action according to strategy\n",
    "        action = epsilon_greedy_action_selection(model, epsilon, observation)\n",
    "        \n",
    "        # Perform action and get next state\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        next_observation = next_observation.reshape([1, 4])  # Reshape!\n",
    "        \n",
    "        replay_buffer.append((observation, action, reward, next_observation, done))  # Update the replay buffer\n",
    "        \n",
    "        observation = next_observation  # Update the observation\n",
    "        \n",
    "        points += 1\n",
    "\n",
    "        # Train the model by replaying\n",
    "        #print(f\"*** Debug: Done = {done}\")\n",
    "        replay(replay_buffer, 32, model, target_model)\n",
    "    epsilon *= EPSILON_REDUCE  # Reduce epsilon\n",
    "    \n",
    "    # Check if we need to update the target model\n",
    "    update_model_handler(epoch, update_target_model, model, target_model)\n",
    "    \n",
    "    if points > best_so_far:\n",
    "        best_so_far = points\n",
    "    if epoch %25 == 0:\n",
    "        print(f\"========== {epoch}: Points reached: {points} - epsilon: {epsilon} - Best: {best_so_far}\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploit learned Q values in test simulations\n",
    "\n",
    "Finally, let's showcase how the trained agent performs by graphically vizualizing it behaving in the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\") \n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for counter in range(500):\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "    # Choose action from predicted Q-values\n",
    "    action = np.argmax(model.predict(observation.reshape([1,1,4]), verbose=0)) \n",
    "    \n",
    "    # Perform the action \n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # clear_output(wait=True)\n",
    "    \n",
    "    if done:\n",
    "        print(f\"Test episode done\")\n",
    "        observation, info = env.reset()\n",
    "        #break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you everyone!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a6e69",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydqn",
   "language": "python",
   "name": "pydqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
