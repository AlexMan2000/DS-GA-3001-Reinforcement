{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ccd19fe",
   "metadata": {},
   "source": [
    "### CDS NYU\n",
    "### DS-GA 3001 | Reinforcement Learning\n",
    "### Lab 07\n",
    "### March 13, 2025\n",
    "\n",
    "\n",
    "# Implement Custom Gym RL Environments\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d60a3b",
   "metadata": {},
   "source": [
    "## Professor\n",
    "Jeremy Curuksu, PhD -- jeremy.cur@nyu.edu\n",
    "\n",
    "## Section Leader\n",
    "Akshitha Kumbam – ak11071@nyu.edu\n",
    "\n",
    "Kushagra Khatwani – kk5395@nyu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e449e3b",
   "metadata": {},
   "source": [
    "## Goal of Today's Lab \n",
    "\n",
    "In this Lab, we will implement custom RL environments by building on existing Gym environments. We will see how to change the state-space and/or action-space definition, how to import custom data to redefine the environment, and how to register a new environment in Gym so as to continue leveraging Gym capabilities for a custom RL problem. \n",
    "\n",
    "Today we will customize pre-existing Gym environments, but we could also get rid of Gym entirely and develop an environment from scratch. There are other RL frameworks available too, which we will discuss later this semester (for example: Google DeepMind ACME, Amazon SageMaker RL, Facebook Meta ReAgent, etc). Note these framework tend to specialize in some functionalities such as specific types of agents, distributed computing, etc, and are often compatible with Gym. Gym remains the most widely used standard benchmark of RL environments.\n",
    "\n",
    "Gym has lots to offer to avoid reinventing the wheel. Using Gym can help make sure you (the developer) focus your time and effort on what is truly new, on the innovative RL problem you are trying to solve. But at this time in the course you should also start getting comfortable thinking about how you would create a custom environment for *any kind* of RL problem, whichever interests you the most. For the project in this course, you are free to use Gym or not, and/or any other RL framework.\n",
    "\n",
    "\n",
    "We will cover three case studies today, the first of which (Gridlock) is the official Gym tutorial available at the Gymnasium documentation link shown below. Then we will turn to develop a custom stock trading agent and a custom state-action space for a Super Mario Bros agent.\n",
    "\n",
    "## Resources\n",
    "\n",
    "* https://gymnasium.farama.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306bad5",
   "metadata": {},
   "source": [
    "# 1. Implement a custom Gridworld environment in Gym\n",
    "\n",
    "This case study comes directly from the official Gym documentation tutorial, which you can find here: https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/\n",
    "\n",
    "Since we will go all the way from creating a new Gym environment in OOP to registering it in Gymnasium as a Python package, we will write code in scripts rather than the Jupyter Notebook and use a standard dev structure with the following hierarchy of folders and files, please bring this entire structure of folders/files on your computer:\n",
    "\n",
    "`gym-examples/`<br>\n",
    "  &emsp;&emsp;`README.md`<br>\n",
    "  &emsp;&emsp;`setup.py`<br>\n",
    "  &emsp;&emsp;`gym_examples/`<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;`__init__.py`<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;`envs/`<br>\n",
    "      &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`__init__.py`<br>\n",
    "      &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`grid_world.py`<br>\n",
    "      \n",
    "In this notebook we use `pygmentize` to read each script, but feel free to read the scripts outside Jupyter Notebook using your favorite Python code editor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40827e5f",
   "metadata": {},
   "source": [
    "The custom Gridlock environment will be a two-dimensional square grid of custom size where the RL agent can move vertically or horizontally between grid cells at each timestep. The goal of the agent is to navigate to a target (red square) on the grid that is placed randomly at the beginning of each episode.\n",
    "\n",
    "* The observation (state) is the location of the target and the agent\n",
    "\n",
    "* The agent can take one of 4 possible actions at each step corresponding to the movements “right”, “up”, “left”, or “down”\n",
    "\n",
    "* Done is set to `true` (episode terminated) as soon as the agent has navigated to the grid cell where the target is located\n",
    "\n",
    "* Rewards are binary and sparse, meaning that the immediate reward is always zero, unless the agent reaches the target where the reward is +1\n",
    "\n",
    "* An episode in this environment (with size=5) might look like this:\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./CustomGridSize5.png\" width=\"500\">\n",
    "\n",
    "<br>\n",
    "\n",
    "where the blue dot is the agent and the red square is the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f23eaa",
   "metadata": {},
   "source": [
    "##  Write an environment custom class which inherits from the parent class `gym.Env`\n",
    "\n",
    "The custom environment class with have seven components:\n",
    "\n",
    "1. Initialization of attributes: `__init__()`\n",
    "2. Construction of observations from the environment state: `_get_obs()`\n",
    "3. Auxiliary information: `_get_info()`\n",
    "4. Reset method: `reset()`\n",
    "5. Step method: `step()`\n",
    "6. Render method: `render()`/`_render_frame()`\n",
    "7. Close method: `close()`\n",
    "\n",
    "\n",
    "This is a copy-paste from the code provided in the Gymnasium tutorial. You can read explanation and description of the code through the comments inserted in the script itself, or in better-looking, HTML format directly from the doc: https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0583affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pygmentize gym-examples/gym_examples/envs/grid_world.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a4fec",
   "metadata": {},
   "source": [
    "In other environments, `close()` might also close files that were opened or release other resources. You shouldn’t interact with the environment after having called close."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6371d7",
   "metadata": {},
   "source": [
    "## Register the environment in Gym\n",
    "\n",
    "In order for the custom environment to be detected by Gym, it must be registered as follows in the file ``gym-examples/gym_examples/__init__.py``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pygmentize gym-examples/gym_examples/__init__.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff243c8",
   "metadata": {},
   "source": [
    "The environment ID consists of three components, two of which are\n",
    "optional: an optional namespace (here: ``gym_examples``), a mandatory\n",
    "name (here: ``GridWorld``) and an optional but recommended version\n",
    "(here: v0). It might have also been registered as ``GridWorld-v0`` (the\n",
    "recommended approach), ``GridWorld`` or ``gym_examples/GridWorld``, and\n",
    "the appropriate ID should then be used during environment creation.\n",
    "\n",
    "The keyword argument ``max_episode_steps=300`` will ensure that\n",
    "GridWorld environments that are instantiated via ``gymnasium.make`` will\n",
    "be wrapped in a ``TimeLimit`` wrapper (see `the wrapper documentation </api/wrappers>`__ for more information). \n",
    "A done signal will then be produced if the agent has reached the target *or* 300 steps\n",
    "have been executed in the current episode. To distinguish truncation and\n",
    "termination, you can check ``info[\"TimeLimit.truncated\"]``.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d17540d",
   "metadata": {},
   "source": [
    "## Create a Python package\n",
    "\n",
    "The last step is to structure our code as a Python package. This\n",
    "involves configuring ``gym-examples/setup.py``. A minimal example of how\n",
    "to do so is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pygmentize gym-examples/setup.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23a77c",
   "metadata": {},
   "source": [
    "## Install the custom package\n",
    "\n",
    "Install the package locally (``pip install -e gym-examples`` needs be typed where the package is located, outside the package tree of files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad951af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e gym-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d686b",
   "metadata": {},
   "source": [
    "## Create an instance of the custom environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa0ec1",
   "metadata": {},
   "source": [
    "Now let's simulate and vizualize the environment we created! Given we registered our custom environment in Gym, we can create an instance of the environment via all our usual Gym commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad77c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_examples\n",
    "\n",
    "# Load custom environment we created \n",
    "env = gym.make('gym_examples/GridWorld-v0', render_mode = \"human\", size=5) \n",
    "\n",
    "# Set to initial state\n",
    "env.reset()  \n",
    "\n",
    "# Loop over 200 steps\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()                           # Choose a random action\n",
    "    new_state, reward, done, truncated, info = env.step(action)  # Carry out the action\n",
    "    \n",
    "    if done:\n",
    "         env.reset()\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdce887",
   "metadata": {},
   "source": [
    "# 2. Exercises: Implement your own custom financial trading environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02baadf9",
   "metadata": {},
   "source": [
    "## Example solution: \n",
    "An example of custom trading environment was implemented in the previous lab, where we added engineered feature time series to the agent state defined by financial indicators called `momentum_rsi`, `volume_obv`, `trend_macd_diff` computed by the `ta` external library. Any indicator computed by this library can be added to the state space, defining a new custom trading environment (solution below). \n",
    "\n",
    "In fact, just downloading a specific set of stocks for specific training/testing timelines becomes a new, custom financial trading environment for the agent to explore and learn from.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576cc626",
   "metadata": {},
   "source": [
    "### Set up the S&P 500 stock index trading environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e34c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick fix for M1 architecture (M2/M3 might also need this). This fix is for importing stable baselines which fails on Apple Silicon for some versions.\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27995453",
   "metadata": {},
   "source": [
    "For some Apple Silicon architectures, stable-baselines3 causes a segmentation fault when attempting to load the model on CPU. It is advisable to use the GPU in the form of mps(Metal Performance Shaders) which is a hacky fix. You might have to use mps while we try to find an actual fix. Uncomment the code below if your kernel crashes while trying to train the A2C model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d187fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS ONLY IF YOU'RE ON APPLE SILICON (And/or want to switch to Apple's GPU)\n",
    "# import torch\n",
    "# if torch.backends.mps.is_available():\n",
    "#     mps_device = torch.device(\"mps\")\n",
    "#     x = torch.ones(1, device=mps_device)\n",
    "#     print (x)\n",
    "# else:\n",
    "#     print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv \n",
    "from stable_baselines3 import A2C \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym_anytrading  # https://github.com/AminHP/gym-anytrading\n",
    "from gym_anytrading.envs import StocksEnv\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "yf.pdr_override() # bug fix, for details see https://stackoverflow.com/questions/74862453/why-am-i-getting-a-typeerror-string-indices-must-be-integer-message-when-tryi\n",
    "from ta import add_all_ta_features # Method from TA (Technical Analysis) library to engineer financial indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pdr.get_data_yahoo('SPY', start='2021-01-01', end='2023-01-01')\n",
    "# Engineer financial indicators using the method imported above from the \"TA\" library\n",
    "df2 = add_all_ta_features(data, open='Open', high='High', low='Low', close='Close', volume='Volume', fillna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c74353",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab95e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_processed_data(env):\n",
    "    start = env.frame_bound[0] - env.window_size\n",
    "    end = env.frame_bound[1]\n",
    "    prices = env.df.loc[:, 'Low'].to_numpy()[start:end]\n",
    "    signal_features = env.df.loc[:, ['Close', 'Volume', 'momentum_rsi', 'volume_obv', 'trend_macd_diff']].to_numpy()[start:end]\n",
    "    return prices, signal_features\n",
    "\n",
    "class MyCustomEnv(StocksEnv):\n",
    "    _process_data = my_processed_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = MyCustomEnv(df=df2, window_size= 5, frame_bound=(5, 400))\n",
    "print(env2.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79562b29",
   "metadata": {},
   "source": [
    "### Train the `A2C` algorithm from `stable-baselines` with the engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C('MlpPolicy', env2, verbose=1)\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae1139f",
   "metadata": {},
   "source": [
    "### Exploit the trained agent in test simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9270c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyCustomEnv(df=df2, window_size=5, frame_bound=(400, 502))\n",
    "obs = env.reset()\n",
    "obs = obs[0]\n",
    "\n",
    "while True:\n",
    "    action, states = model.predict(obs)\n",
    "    obs, rewards, term, trunc, info = env.step(action)\n",
    "    if term or trunc:\n",
    "        print(info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66980a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.cla()\n",
    "env.render_all()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb3dfa",
   "metadata": {},
   "source": [
    "## Second example solution: \n",
    "The agent state can be defined based on forecasts obtained from a RNN deep learning trained on trends in recent stock prices. A solution for this can be found at the following AWS Blog Post (a bit outdated, it was published in 2018): https://aws.amazon.com/blogs/machine-learning/forecasting-time-series-with-dynamic-deep-learning-on-aws/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf63c14",
   "metadata": {},
   "source": [
    "## Thank you everyone!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydqn",
   "language": "python",
   "name": "pydqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
