{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDS NYU\n",
    "### DS-GA 3001 | Reinforcement Learning\n",
    "### Lab 03\n",
    "### February 13, 2025\n",
    "\n",
    "\n",
    "# Value Iteration and Policy Iteration algorithms\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Leader\n",
    "\n",
    "\n",
    "Akshitha Kumbam – ak11071@nyu.edu\n",
    "\n",
    "Kushagra Khatwani – kk5395@nyu.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8ea82",
   "metadata": {},
   "source": [
    "## Goal of Today's Lab \n",
    "\n",
    "In this Lab, we will implement RL algorithms which learn value functions and derive an optimal policy from the learned values, assuming a model of the environment is available. Next week we will extend these algroithms to cases where a model of the environment is not available.\n",
    "\n",
    "In the first case study, we will build the entire agent-environment interface from scratch. This will be mostly theoretical. In the second case study, we will return to Gym, invoking an environment very similar to the one in the first case study, so you can see the similarities, but also to start using the nive features of Gym such as a graphical rendering of the environment. In future labs, we will systematically use Gym so as to work with more complex environments, and so as to focus our efforts more on building RL agents than on building environments. Today we cover the most fundamental RL agent paradigm, which assumes a model of the environment is available. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2398d06d",
   "metadata": {},
   "source": [
    "# 1. Implement a value iteration algorithm \n",
    "\n",
    "Consider the following learning problem. A bear (the agent) wants to get to the honey (the positive reward) while avoiding the bees around it (negative rewards). At every state (a cell in a 3 $\\times$ 4 grid), the bear can take an action such as go up, down, left or right. \n",
    "\n",
    "The model for this use case is fully known and represented by state transition probabilities where any of the adjacent cells can be selected randomly with a probablity of 10% (parameter `NOISE` below). The bear takes actions according to its own policy except in 10% of cases where it ends up in a randomly selected adjacent cell. For example, assume the bear is in a rugged terrain such that 10% of the time it missteps and doesn’t control where it is going. This model implies that it is more “risky” to be closer to the bees because a mistep can lead to a negative reward.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./BearBeesHoney.png\" width=\"400\">\n",
    "\n",
    "<br>\n",
    "\n",
    "In order to find the best way to get to the honey, the bear will walk in the environment. He might walk into the bees, get stung, and learn to avoid this state. Eventually, with time, he will figure out how to behave in any given state, to get to the honey without getting stung.\n",
    "\n",
    "But how will the bear learn this? The value iteration algorithm is one of the most common algorithm to solve this problem. Let’s get into it.\n",
    "\n",
    "https://towardsdatascience.com/how-to-code-the-value-iteration-algorithm-for-reinforcement-learning-8fb806e117d1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa424b9",
   "metadata": {},
   "source": [
    "### Set up an environment for the Markov Decision Process (states, rewards, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a523e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "SMALL_ENOUGH = 0.005 # Threshold used to stop learning when max difference in learned values between two iteration is small enough\n",
    "GAMMA = 0.9         \n",
    "NOISE = 0.10  \n",
    "\n",
    "# Define all states\n",
    "all_states=[]\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "            all_states.append((i,j))\n",
    "\n",
    "# Define rewards for all states\n",
    "rewards = {}\n",
    "for i in all_states:\n",
    "    if i == (1,2):\n",
    "        rewards[i] = -1\n",
    "    elif i == (2,2):\n",
    "        rewards[i] = -1\n",
    "    elif i == (2,3):\n",
    "        rewards[i] = 1\n",
    "    else:\n",
    "        rewards[i] = 0\n",
    "\n",
    "# Dictionnary of possible actions for all but the \"end\" states (1,2 and 2,2 and 2,3)\n",
    "actions = {\n",
    "    (0,0):('D', 'R'), \n",
    "    (0,1):('D', 'R', 'L'),    \n",
    "    (0,2):('D', 'L', 'R'),\n",
    "    (0,3):('D', 'L'),\n",
    "    (1,0):('D', 'U', 'R'),\n",
    "    (1,1):('D', 'R', 'L', 'U'),\n",
    "    (1,3):('D', 'L', 'U'),\n",
    "    (2,0):('U', 'R'),\n",
    "    (2,1):('U', 'L', 'R'),\n",
    "    }\n",
    "\n",
    "# Initialize a policy for non-final states\n",
    "policy={}\n",
    "for s in actions.keys():\n",
    "    policy[s] = np.random.choice(actions[s])\n",
    "\n",
    "# Initialize value function \n",
    "# Make sure to assign value of final states with the known reward, those will not be \"learned\"\n",
    "V={}\n",
    "for s in all_states:\n",
    "    if s in actions.keys():\n",
    "        V[s] = 0\n",
    "    if s ==(2,2):\n",
    "        V[s]=-1\n",
    "    if s == (1,2):\n",
    "        V[s]=-1\n",
    "    if s == (2,3):\n",
    "        V[s]=1\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8194d34b",
   "metadata": {},
   "source": [
    "### 1.1 Simulate a value iteration algorithm to update  state values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    biggest_change = 0\n",
    "    for s in all_states:            \n",
    "        if s in policy:\n",
    "            \n",
    "            old_v = V[s]\n",
    "            new_v = float('-inf')  # Initialize new_v with a very low value\n",
    "\n",
    "            for a in actions[s]:\n",
    "                # Determine the next state for the intended action\n",
    "                if a == 'U':\n",
    "                    nxt = [s[0]-1, s[1]]\n",
    "                if a == 'D':\n",
    "                    nxt = [s[0]+1, s[1]]\n",
    "                if a == 'L':\n",
    "                    nxt = [s[0], s[1]-1]\n",
    "                if a == 'R':\n",
    "                    nxt = [s[0], s[1]+1]\n",
    "                \n",
    "                # Intended action value (with (1 - NOISE) probability)\n",
    "                nxt = tuple(nxt)  # Convert to tuple\n",
    "                intended_value = (1 - NOISE) * V[nxt]\n",
    "\n",
    "                # Now we calculate the value for random alternative actions\n",
    "                possible_moves = [x for x in actions[s]]  # All available actions\n",
    "                random_value = 0\n",
    "                if possible_moves:\n",
    "                    probability_per_alt = NOISE / len(possible_moves)  # Evenly distribute noise\n",
    "\n",
    "                    for random_a in possible_moves:\n",
    "                        if random_a == 'U':\n",
    "                            act = [s[0]-1, s[1]]\n",
    "                        if random_a == 'D':\n",
    "                            act = [s[0]+1, s[1]]\n",
    "                        if random_a == 'L':\n",
    "                            act = [s[0], s[1]-1]\n",
    "                        if random_a == 'R':\n",
    "                            act = [s[0], s[1]+1]\n",
    "                        \n",
    "                        act = tuple(act)\n",
    "                        random_value += probability_per_alt * V[act]  # Sum up contributions from all alternatives\n",
    "\n",
    "                # Calculate the total value for the state using the Bellman equation\n",
    "                v = rewards[s] + GAMMA * (intended_value + random_value)\n",
    "\n",
    "                # Update the best value for this state\n",
    "                if v > new_v:\n",
    "                    new_v = v\n",
    "                    policy[s] = a  # Update the policy to the best action\n",
    "\n",
    "            # Save the best value for the state\n",
    "            V[s] = new_v\n",
    "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    if iteration % 5000 == 0: \n",
    "        print(f\"Step {iteration}, Biggest change = {biggest_change}\")\n",
    "\n",
    "    #OPTIONAL PRINTING VALUES\n",
    "    # # Print the current value function (V) after every iteration\n",
    "    # print(\"\\nCurrent Value Function (V) after iteration\", iteration)\n",
    "    # for i in range(3):\n",
    "    #     row = \"\"\n",
    "    #     for j in range(4):\n",
    "    #         row += f\"{V[(i, j)]:.2f}  \"\n",
    "    #     print(row)\n",
    "\n",
    "    # Stop learning if max difference of all values between two iterations is smaller than predefined threshold     \n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "        print(f\"State values converged in exactly {iteration} iterations.\")\n",
    "        break\n",
    "    \n",
    "    iteration += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d1683",
   "metadata": {},
   "source": [
    "### Visualize the Policy and Final values for Value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Final Policy\n",
    "print(\"\\nFinal Policy:\")\n",
    "for i in range(3):\n",
    "    row = \"\"\n",
    "    for j in range(4):\n",
    "        if (i, j) in policy:\n",
    "            row += policy[(i, j)] + \"  \"\n",
    "        else:\n",
    "            row += \"X  \"  # Terminal or blocked states\n",
    "    print(row)\n",
    "\n",
    "    # Print Final Value Function\n",
    "print(\"\\nFinal State Values:\")\n",
    "for i in range(3):\n",
    "    row = \"\"\n",
    "    for j in range(4):\n",
    "        row += f\"{V[(i, j)]:.2f}  \"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2295b279",
   "metadata": {},
   "source": [
    "### 1.2 Simulate a policy iteration algorithm to update  state values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ca512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "SMALL_ENOUGH = 0.005  # Convergence threshold\n",
    "GAMMA = 0.9           # Discount factor\n",
    "NOISE = 0.10          # Probability of taking a random action\n",
    "\n",
    "# Re-initialize policy (random)\n",
    "policy = {s: np.random.choice(actions[s]) for s in actions.keys()}\n",
    "\n",
    "#Re-initialize value function\n",
    "V = {s: 0 for s in all_states}\n",
    "V[(1, 2)], V[(2, 2)], V[(2, 3)] = -1, -1, 1  # Fixed terminal values\n",
    "\n",
    "# Function to get next state\n",
    "def get_next_state(s, a):\n",
    "    i, j = s\n",
    "    if a == 'U':\n",
    "        i -= 1\n",
    "    elif a == 'D':\n",
    "        i += 1\n",
    "    elif a == 'L':\n",
    "        j -= 1\n",
    "    elif a == 'R':\n",
    "        j += 1\n",
    "    return (i, j) if (i, j) in all_states else s  # Stay in bounds\n",
    "\n",
    "# Policy Iteration Algorithm\n",
    "def policy_iteration():\n",
    "    global policy, V\n",
    "    iteration = 0\n",
    "\n",
    "    while True:\n",
    "        # Policy Evaluation Step\n",
    "        while True:\n",
    "            biggest_change = 0\n",
    "            for s in all_states:\n",
    "                if s in policy:  # Non-terminal states\n",
    "                    old_v = V[s]\n",
    "                    a = policy[s]\n",
    "                    nxt = get_next_state(s, a)\n",
    "\n",
    "                    # Stochastic noise distribution: Evenly distribute noise over alternative actions\n",
    "                    possible_moves = [x for x in actions[s]]  # Alternative moves\n",
    "                    random_value = 0\n",
    "                    if possible_moves:\n",
    "                        probability_per_alt = NOISE / len(possible_moves)  # Split noise equally\n",
    "\n",
    "                        for random_a in possible_moves:\n",
    "                            act = get_next_state(s, random_a)\n",
    "                            random_value += probability_per_alt * V[act]  # Weighted sum of all alt actions\n",
    "                    \n",
    "                    # Update value with the intended action and alternative actions\n",
    "                    V[s] = rewards[s] + GAMMA * ((1 - NOISE) * V[nxt] + random_value)\n",
    "                    biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "            if biggest_change < SMALL_ENOUGH:  # Stop if values converged\n",
    "                break\n",
    "\n",
    "        # Policy Improvement Step\n",
    "        policy_stable = True\n",
    "        for s in actions.keys():\n",
    "            old_action = policy[s]\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "\n",
    "            # Evaluate all actions and choose the one with the highest value\n",
    "            for a in actions[s]:\n",
    "                nxt = get_next_state(s, a)\n",
    "\n",
    "                # Stochastic noise distribution for policy improvement\n",
    "                possible_moves = [x for x in actions[s]]\n",
    "                random_value = 0\n",
    "                if possible_moves:\n",
    "                    probability_per_alt = NOISE / len(possible_moves)\n",
    "\n",
    "                    for random_a in possible_moves:\n",
    "                        act = get_next_state(s, random_a)\n",
    "                        random_value += probability_per_alt * V[act]\n",
    "\n",
    "                # Compute the expected value for this action\n",
    "                v = rewards[s] + GAMMA * ((1 - NOISE) * V[nxt] + random_value)\n",
    "\n",
    "                if v > best_value:\n",
    "                    best_value = v\n",
    "                    best_action = a\n",
    "\n",
    "            policy[s] = best_action  # Update policy\n",
    "            if old_action != best_action:\n",
    "                policy_stable = False  # Policy changed\n",
    "\n",
    "        iteration += 1\n",
    "        print(f\"Iteration {iteration}: Policy updated.\")\n",
    "\n",
    "        if policy_stable:\n",
    "            print(\"Policy has converged.\")\n",
    "            break\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "# Run Policy Iteration\n",
    "final_policy, final_values = policy_iteration()\n",
    "\n",
    "# Print Final Policy\n",
    "print(\"\\nFinal Policy:\")\n",
    "for i in range(3):\n",
    "    row = \"\"\n",
    "    for j in range(4):\n",
    "        if (i, j) in final_policy:\n",
    "            row += final_policy[(i, j)] + \"  \"\n",
    "        else:\n",
    "            row += \"X  \"  # Terminal or blocked states\n",
    "    print(row)\n",
    "\n",
    "# Print Final Value Function\n",
    "print(\"\\nFinal State Values:\")\n",
    "for i in range(3):\n",
    "    row = \"\"\n",
    "    for j in range(4):\n",
    "        row += f\"{final_values[(i, j)]:.2f}  \"\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af764088",
   "metadata": {},
   "source": [
    "### [Optional Exercise]: \n",
    "Create some plots to vizualize the evolution of the value $v(s)$ for each state $s$ over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c1e48",
   "metadata": {},
   "source": [
    "# 2. Implement value iteration in a Gym RL simulation environment \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87faee",
   "metadata": {},
   "source": [
    "##  Build a `FrozenLake` Gym environment\n",
    "\n",
    "In the Frozen Lake environment, the agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a target tile.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "\n",
    "    SFFF       (S: starting point, safe)\n",
    "    FHFH       (F: frozen surface, safe)\n",
    "    FFFH       (H: hole: fall to your doom)\n",
    "    HFFG       (G: goal: where the target, let's say a present, is located)\n",
    "\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise. In this case study we will learn value more effectively by penalizing the agent (-1) when falling in a hole.\n",
    "\n",
    "There are up to 4 actions (0,1,2,3) and 16 states (0,1,...,15), please read the concise doc of this Gym environment here: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "We will remove \"slippery\" from the environment to have control on how we define the randomness to the direction. We will simulate randomness as taking a random direction 10%of the time. Of course feel free to add back the slippery feature and experiment with learning strategies, but this is a much more challenging problem to solve (the agent will move in intended direction only with probability of 1/3, see above doc for details) \n",
    "\n",
    "Finally, depending on your computer specs or patience, you can scale the size of the lake up or down:\n",
    "\n",
    "https://stackoverflow.com/questions/55006689/how-to-generate-a-random-frozen-lake-map-in-openai\n",
    "\n",
    "To keep things simple, we will use the default 4$\\times$4 grid, but check out the link above if you're crazy enough to go to some huge N$\\times$N grid :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df2e17ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "import ipywidgets\n",
    "from IPython.display import clear_output\n",
    "import time  \n",
    "import gymnasium as gym\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\", is_slippery=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a1390",
   "metadata": {},
   "source": [
    "### Let's run `FrozenLake` with random actions just to get familiar with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad76eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\", is_slippery=False)  \n",
    "env.reset()  \n",
    "for _ in range(50):\n",
    "    env.render()  \n",
    "    action = env.action_space.sample()  # Select a random action\n",
    "    new_state, reward, done, info, p = env.step(action)  # Perform random action on the environment\n",
    "    \n",
    "    if done:\n",
    "         env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae46af8",
   "metadata": {},
   "source": [
    "### Define a model of possible state-action pairs for `FrozenLake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ef30165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rewards for all states\n",
    "rewards = {}\n",
    "for i in range(env.observation_space.n):\n",
    "    if i == 5:\n",
    "        rewards[i] = -1\n",
    "    elif i == 7:\n",
    "        rewards[i] = -1\n",
    "    elif i == 11:\n",
    "        rewards[i] = -1\n",
    "    elif i == 12:\n",
    "        rewards[i] = -1\n",
    "    elif i == 15:\n",
    "        rewards[i] = 1\n",
    "    else:\n",
    "        rewards[i] = 0\n",
    "\n",
    "# Dictionnary of possible actions for each state (all but final states)\n",
    "actions = {\n",
    "    0:('D', 'R'), \n",
    "    1:('D', 'R', 'L'),    \n",
    "    2:('D', 'L', 'R'),\n",
    "    3:('D', 'L'),\n",
    "    4:('D', 'U', 'R'),\n",
    "    6:('D', 'R', 'L', 'U'),\n",
    "    8:('D', 'U', 'R'),\n",
    "    9:('D', 'R', 'L', 'U'),\n",
    "    10:('D', 'R', 'L', 'U'),    \n",
    "    13:('U', 'L', 'R'),\n",
    "    14:('U', 'L', 'R')\n",
    "    }\n",
    "\n",
    "# Initialize policy\n",
    "policy={}\n",
    "for s in actions.keys():\n",
    "    policy[s] = np.random.choice(actions[s])\n",
    "    \n",
    "# Initialize value function \n",
    "# Make sure to assign value of final states with known reward, those don't need to be learned\n",
    "V={}\n",
    "for s in range(env.observation_space.n):\n",
    "    if s in actions.keys():\n",
    "        V[s] = 0\n",
    "    if s == 5:\n",
    "        V[s]= -1\n",
    "    if s == 7:\n",
    "        V[s]= -1\n",
    "    if s == 11:\n",
    "        V[s]= -1\n",
    "    if s == 12:\n",
    "        V[s]= -1\n",
    "    if s == 15:\n",
    "        V[s]= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8a70e",
   "metadata": {},
   "source": [
    "### Update the state values using the value iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd61ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "SMALL_ENOUGH = 0.05\n",
    "GAMMA = 0.9         \n",
    "NOISE = 0.20\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    biggest_change = 0\n",
    "    for s in range(env.observation_space.n):   \n",
    "        if s in policy:\n",
    "            \n",
    "            old_v = V[s]\n",
    "            new_v = 0\n",
    "            \n",
    "            for a in actions[s]:\n",
    "                # Select next state correpsonding to action a\n",
    "                if a == 'L': # Left\n",
    "                    nxt = s - 1 # Next state when moving to the left\n",
    "                if a == 'D': # Down\n",
    "                    nxt = s + 4 # Next state when moving down\n",
    "                if a == 'R': # Right\n",
    "                    nxt = s + 1 # Next state when moving to the right\n",
    "                if a == 'U': # Up\n",
    "                    nxt = s - 4 # Next state when moving up\n",
    "\n",
    "                # Intended action value (with (1 - NOISE) probability)\n",
    "                nxt = tuple([nxt])  # Convert to tuple for uniformity\n",
    "                intended_value = (1 - NOISE) * V[nxt[0]]\n",
    "\n",
    "                # Now calculate the value for random alternative actions\n",
    "                possible_moves = [x for x in actions[s]]  # All available actions\n",
    "                random_value = 0\n",
    "                if possible_moves:\n",
    "                    probability_per_alt = NOISE / len(possible_moves)  # Evenly distribute noise\n",
    "\n",
    "                    for random_a in possible_moves:\n",
    "                        if random_a == 'L':  # Left\n",
    "                            act = s - 1 if s % 4 != 0 else s\n",
    "                        if random_a == 'D':  # Down\n",
    "                            act = s + 4 if s + 4 < 16 else s\n",
    "                        if random_a == 'R':  # Right\n",
    "                            act = s + 1 if (s + 1) % 4 != 0 else s\n",
    "                        if random_a == 'U':  # Up\n",
    "                            act = s - 4 if s - 4 >= 0 else s\n",
    "                        \n",
    "                        act = tuple([act])\n",
    "                        random_value += probability_per_alt * V[act[0]]  # Sum up contributions from all alternatives\n",
    "                # Calculate the total value for the state using the Bellman equation\n",
    "                v = rewards[s] + GAMMA * (intended_value + random_value)\n",
    "                if v > new_v: # If this the best action so far, update the policy with this action\n",
    "                    new_v = v\n",
    "                    policy[s] = a\n",
    "  \n",
    "            # Save the best of all actions for the state                                \n",
    "            V[s] = new_v\n",
    "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))   \n",
    "            \n",
    "    if iteration % 5000 == 0: \n",
    "        print(\"Episode {}, Biggest change = {}\".format(iteration, biggest_change))\n",
    "\n",
    "    # Stop learning if max difference in v value between two iteration is smaller than predefined threshold          \n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "        print(\"State values converged in {} iteratons.\".format(iteration))\n",
    "        # Let's look at the learned values\n",
    "        plt.plot(range(0, 16), list(V.values()), 'o-', color = [0.3, 0, 0.6], linewidth=3, markersize = 10)\n",
    "        plt.xlabel('State')\n",
    "        plt.ylabel('Learned Value');\n",
    "        plt.title('Final values after convergence')\n",
    "        break\n",
    "    iteration += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploit the state values found by the value iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# Play 10 games\n",
    "for episode in range(10):\n",
    "    \n",
    "    # Reset the environment\n",
    "    state,p = env.reset()  \n",
    "                           \n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        env.render()  # Use Gym to vizualize a nice-looking environment\n",
    "        \n",
    "        # Follow the learned policy for current state except from time to time (NOISE) \n",
    "        be_greedy = np.random.random() > NOISE\n",
    "        if be_greedy:\n",
    "            a = policy[state]\n",
    "        else:\n",
    "            a = np.random.choice([i for i in actions[state]])\n",
    "        # Map our custom definition of L,D,R,U to Gym's action space code: 0,1,2,3\n",
    "        if a == 'L': # Left\n",
    "            action = 0\n",
    "        if a == 'D': # Down\n",
    "            action = 1\n",
    "        if a == 'R': # Right\n",
    "            action = 2\n",
    "        if a == 'U': # Up\n",
    "            action = 3           \n",
    "\n",
    "        # Take the action and observe the resulting state and reward \n",
    "        new_state, reward, done, info, p = env.step(action)\n",
    "\n",
    "        # Track rewards\n",
    "        total_rewards = total_rewards + reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    " \n",
    "    episode += 1\n",
    "    # rewards.append(total_rewards)\n",
    "    \n",
    "env.close() # Comment to fix error \"display Surface quit\", this will let the env opened "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional Exercice: ] \n",
    "Train the agent for a more challenging problem by increasing the amount of randomness, and/or changing the slippery feature to `True`, and/or increasing the size of the FrozenLake grid. Feel free to experiment with any learning strategy and custom rules/models that make sense to you. Starting next week, we will implement more general, model-free RL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you everyone!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_final",
   "language": "python",
   "name": "py39_final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
