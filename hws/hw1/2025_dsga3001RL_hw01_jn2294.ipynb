{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 (10 points):  List three key differences between supervised learning and reinforcement learning:\n",
    "\n",
    "<span style=\"color: lightgreen;\" >\n",
    "<ul style=\"display: flex; flex-direction: column; gap: 10px;\">\n",
    "<li style=\"list-style-type: numeber;\"> Supervised learning uses labeled training data with correct input-output pairs to train the model. Reinforcement learning uses no labeled data and that the agent learns from rewards/penalties through interaction with the environment.\n",
    "<li style=\"list-style-type: number;\"> Supervised learning is a one-step process where the model learns direct mappings from inputs to outputs while reinforcement learning is a sequential decision-making process where current actions do affect future states and rewards.\n",
    "<li style=\"list-style-type: number;\"> Supervised learning aims to minimize the KL divergence between the predicted and true distributions(equivalent to maximize the log-likelihood) while reinforcement learning aims to maximize expected or cumulative rewards over time through trial and error, it's like exploring the environment and learning from the feedback.\n",
    "</ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 (5 points): What is the name of the function that maps specific states to specific actions?\n",
    "\n",
    "<span style=\"color: lightgreen;\" >\n",
    "Policy function. This function could be deterministic or stochastic.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 (5 points): What is a possible difference between what an RL agent observes and what defines its state at a given time step?\n",
    "\n",
    "<span style=\"color: lightgreen;\" >\n",
    "The dimension of the state space could be different from the dimension of the observation space. For example, the state space of a autonomous car could be defined as the position and velocity of the car since it minimally describes the car's state at any time step, while the observation could be that the car is moving forward. Generally speaking, the state space is the minimal set of variables that can describe the system, while the observation space is the set of variables that the agent can observe. In other words, the observation space can oftenly be partial compared to the full state space.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 (5 points): What is the difference between a deterinistic policy and a stochastic policy?\n",
    "\n",
    "<span style=\"color: lightgreen;\" >\n",
    "A deterministic policy maps each state to a single action, while a stochastic policy maps each state to a distribution over actions.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 (5 points): For each of the following scenarios, determine whether the given policy is deterministic or stochastic. Briefly justify your answer.\n",
    "\n",
    "<div style=\"display: flex; flex-direction: column; gap: 0px;\">\n",
    "(a) Scenario: A robot moves in a grid world.\n",
    "\n",
    "<span style=\"padding-left: 26px;\">Policy: The robot always moves right if possible; otherwise, it moves up.\n",
    "</span>\n",
    "\n",
    "<span style=\"padding-left: 26px; color: lightgreen;\">Answer: Deterministic policy. Since there is no randomness in the action taken, all the next step move is predictable.</span>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"display: flex; flex-direction: column; gap: 0px;\">\n",
    "(b) Scenario: A self-driving car is approaching an intersection.\n",
    "\n",
    "<span style=\"padding-left: 26px;\">Policy: The car chooses to turn left with 70% probability and right with 30% probability.\n",
    "</span>\n",
    "\n",
    "<span style=\"padding-left: 26px; color: lightgreen;\">Answer: Stochastic policy. Since at each state, the car can choose from two actions with certain probabilities, which you cannot predict.</span>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"display: flex; flex-direction: column; gap: 0px;\">\n",
    "(c) Scenario: A chess engine selects a move given a specific board state.\n",
    "\n",
    "<span style=\"padding-left: 26px;\">Policy: The engine always picks the move with the highest evaluation score.\n",
    "</span>\n",
    "\n",
    "<span style=\"padding-left: 26px; color: lightgreen;\">Answer: Deterministic policy. It's fully rule-based and is predictable.</span>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"display: flex; flex-direction: column; gap: 0px;\">\n",
    "(d) Scenario: A robot is navigating through rough terrain.\n",
    "\n",
    "<span style=\"padding-left: 26px;\">Policy: The robot chooses an action based on a probability distribution over safe movements.\n",
    "</span>\n",
    "\n",
    "<span style=\"padding-left: 26px; color: lightgreen;\">Answer: Stochastic policy. Since at each state, the robot can choose from multiple actions with certain probabilities, which you cannot predict.</span>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"display: flex; flex-direction: column; gap: 0px;\">\n",
    "(e) Scenario: A thermostat controls room temperature.\n",
    "\n",
    "<span style=\"padding-left: 26px;\">Policy: It turns the heater on when the tempature drops below 18°C and off when it rises above 22°C.\n",
    "</span>\n",
    "\n",
    "<span style=\"padding-left: 26px; color: lightgreen;\">Answer: Deterministic policy. It is fully rule-based and is predictable. Precisely, there is a injective mapping between the current state and the action.</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 (5 points): Define in plain english what is $v(s)$, and what is $q(s, d)$?\n",
    "\n",
    "<span style=\"color: lightgreen;\" >\n",
    "\n",
    "$v(s)$ is the value function of state $s$, which is the expected cumulative reward from state $s$ onwards, which indicates how good it is to be in state $s$ when following the current policy.\n",
    "</span>\n",
    "\n",
    "<span style=\"color: lightgreen;\" >\n",
    "\n",
    "$q(s, a)$ is the value function of state-action pair $(s, a)$, which is the expected cumulative reward from state $s$ and action $a$ onwards, which indicates how good it is to choose action $a$ in state $s$ under current policy.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 (5 points): What is the Bellman equation for $v(s)$ given a discount factor $\\gamma$?\n",
    "\n",
    "<span style=\"color: lightgreen;\" >\n",
    "\n",
    "The Bellman equation for $v(s)$ given a discount factor $\\gamma$ is: \n",
    "\n",
    "$v(s) = \\mathbb{E} [r_{t+1} + \\gamma v(s_{t+1}) | s]$ where $s$ is the current state and $r_{t+1}$ is the reward at time $t+1$.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 (10 points): For an $\\epsilon$-greedy action selection method with only two actions and $\\epsilon = 0.5$, what is the probability that the greedy action is selected?\n",
    "\n",
    "<span style=\"color: lightgreen;\" >\n",
    "\n",
    "For an $\\epsilon$-greedy policy, the probability of choosing greedy action is $ 1- \\epsilon + \\frac{\\epsilon}{2} = 0.75$ (Given by the formula: $1- \\epsilon + \\frac{\\epsilon}{N}$ where $N$ is the number of possible actions to choose from).\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.2 (5 points):  What is the key difference between a Bandit $vs.$ a more general RL problem?\n",
    "\n",
    "<span style=\"color: lightgreen;\" >\n",
    "\n",
    "In the MAB problem, the agent only has to make a single decision at each time step, while in a more general RL problem, the agent has to make a sequence of decisions over time. \n",
    "\n",
    "Moreover, MAB only has one state, past actions won't affect future states choices and rewards while a general RL problem has multiple states and current actions will affect future states choices and rewards.\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 (5 points): What is the key difference between a Contextual Bandit and a more general RL problem?\n",
    "\n",
    "<span style=\"color: lightgreen;\" >\n",
    "In a Contextual Bandit problem, the current action depends on the current state(more than one states, different from MAB) and the action won't affect what next states can be accessed later. \n",
    "\n",
    "In a more general RL problem, the current action can affect what next states can be accessed later and thus the future rewards.\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (20 points)\n",
    "\n",
    "### In the 10-armed Testbed Bandit problem presented in the lecture (also detailed in Chapter 2, Section 2.3 of the Sutton & Barto book):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 (15 points): Which action-selection method, amongst the three options listed below,\n",
    "will perform best in the long run in term of cumulative reward and probability of selecting the\n",
    "best action? These methods are visualized in Fig 2.3 in the book and slide 14 in the lecture.\n",
    "1. $\\epsilon$-greedy with $\\epsilon = 0.1$\n",
    "2. $\\epsilon$-greedy with $\\epsilon = 0.01$\n",
    "3. $\\epsilon$-greedy with $\\epsilon = 0$ (greedy policy) and initial $q$-values set to −10\n",
    "Explain your answer\n",
    "\n",
    "<span style=\"color: lightgreen;\" >\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 (5 points): Suppose we have a 10-armed bandit problem similar to above but where the\n",
    "reward for each of the 10 arms is deterministic (= always same reward for a given arm) and\n",
    "in the range (-5, +5). Same question as 3.1 but for the following action-selection methods:\n",
    "1. $\\epsilon$-greedy with $\\epsilon = 0.1$\n",
    "2. $\\epsilon$-greedy with $\\epsilon = 0.01$\n",
    "3. $\\epsilon$-greedy with $\\epsilon = 0$ (greedy policy) and initial $q$-values set to −10\n",
    "4. $\\epsilon$-greedy with $\\epsilon = 0$ (greedy policy) and initial $q$-values set to +10\n",
    "Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
