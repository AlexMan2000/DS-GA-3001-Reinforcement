{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (30 points)\n",
    "\n",
    "### Function Approximation in Reinforcement Learning:\n",
    "-  1.1 (5 points): What is the difference between tabular Q-learning and Q-learning with function approximation?\n",
    "- 1.2 (5 points): Why are TD updates by Gradient Descent called Semi-Gradient updates?\n",
    "- 1.3 (10 points): What challenges may tabular Q-learning meet when learning to play the game of Chess? Provide at least 2 answers.\n",
    "- 1.4 (10 points): What challenges may value function approximation meet in a Reinforcement Learning context compared to offline regression in a Supervised Learning context? Provide at least 3 answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: lightgreen;\" >\n",
    "\n",
    "- 1.1 **Tabular Q-learning** stores Q-values in a lookup table for every state-action pair, making it simple and effective for small, discrete environments but impractical for large or continuous spaces. **Q-learning with function approximatio** replaces the table with a parameterized function (typically a neural network) to estimate Q-values, allowing it to handle large or continuous spaces and generalize across unseen states, but at the cost of increased complexity and potential instability.\n",
    "- 1.2 The TD value function approximation target is $r_{t+1} + \\gamma v_{w}(s_{t+1})$ so typically the gradient of the squared TD error would be $\\nabla_{w}(r_{t+1}+\\gamma v_{w}(s_{t+1;w})-v_{w}(s_{t};w))$. But semi-gradient approach choose to use $\\nabla_{w}v_{w}(s_{t};w)$ as the gradient instead of both.\n",
    "- 1.3 Answer:\n",
    "    - Chess has an enormous number of possible board states so it is comutationally and memory-wise infeasible. \n",
    "    - The second reason is that each (s, a) pair is treated independently in the across the table. So even if it learns a good strategy in one situation, it cannot transfer that knowledge to similar board positions.\n",
    "- 1.4 Answer:\n",
    "    - In TD value function approximation, the target values depend on model's own predictions(e.g. bootstrapping) where the target value $v_{w}(s_{t+1};w) changes when $w$ is being updated. In contrast in supervised learning, the target value is typically fixed and won't change during training.\n",
    "    - The target values are often biased and correlated samples of the ground truth $v_{\\pi}(s_{t})$. In other words, training data used to parametrize the predictive function is gathered online and often non-stationary, which could result in local optimum or slow convergence.while in supervised learning setting the samples are usually assumed to be i.i.d.\n",
    "    - The environment in RL during exploration and exploitation may continuously evolve, or be too big for exhaustive sampling, and thus may never reach equilibrium whereas in supervised learning the environment is fixed, data is already collected and we don't have to explore.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise 2 (20 points)\n",
    "\n",
    "### Deep Q-Network (DQN):\n",
    "- 2.1 (10 points): What is Experience Replay? How does it benefit DQN?\n",
    "- 2.2 (10 points): Provide a complete pseudo-code for the DQN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: lightgreen;\" >\n",
    "\n",
    "- 2.1 Answer:\n",
    "    - Experience Replay is a method where the agent stores its past experiences (transitions) in a replay buffer: $(s, a, r, s^{'})$. During training, instead of learning from the most recent transition, the agent randomly samples a batch of past experiences from this buffer to update its Q-network.\n",
    "    - Random sampling fromthe replay buffer makes training data more i.i.d, the estimation of value target more stationary which could speed up convergence. Since each experience is used multiple times, they allow the agent the learn more effectively and thoroughly from the past experiences without new interactions with the environment all the time. \n",
    "- 2.2 The pseudocode is as follows:\n",
    "\n",
    "    Initialize:\n",
    "    - Q-network parameters $w$ arbitraraily\n",
    "    - Target Q-network parameters $w_{target} \\leftarrow w$\n",
    "    - Policy $\\pi_{s}$ (e.g. $\\epsilon$-greedy based on $q_{w}(s, a)$)\n",
    "    - Replay buffer $D \\leftarrow$ empty\n",
    "    - Hyperparameters: learning rate $\\alpha$, discount factor $\\gamma$, batch size $B$, target network update frequency $C$\n",
    "    - Step counter $t\\leftarrow 0$\n",
    "\n",
    "    Loop forever:\n",
    "\n",
    "    - Initialize environment and state $s$\n",
    "\n",
    "    - Loop until episode ends:\n",
    "        - Select action $a$ from $s$ using policy $\\pi_{s}$\n",
    "        - Execute $a$, obverse reward $r$ and next state $s'$\n",
    "        - Store the transition $(s, a, r, s')$ in the replay buffer $D$\n",
    "        - Update parameters $w$:\n",
    "            $w \\leftarrow w + \\alpha * (r + \\gamma * \\text{max}_{a'}q_{w}^{target}(s', a') - q_{w}(s, a))\\nabla_{w}q_{w}(s, a)$\n",
    "        - Update policy $\\pi_{s}$ based on $q_{w}(s, a)$ (e.g. with epsilon-greedy)\n",
    "        - Set $s \\leftarrow s'$\n",
    "        - Increment $t \\leftarrow t + 1$\n",
    "\n",
    "        - If $t \\mod C == 0$:\n",
    "            - Sample mini-batches from the replay buffer $D$\n",
    "            - For each $(s_{i}, a_{i}, r_{i}, s_{i+1})$ in $D$:\n",
    "                - Update target network: $w \\leftarrow w + \\alpha * (r_{i} + \\gamma * \\text{max}_{a'}q_{w}^{target}(s_{i+1}, a') - q_{w}(s_{i}, a_{i}))\\nabla_{w}q_{w}(s_{i}, a_{i})$\n",
    "            \n",
    "\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (50 points)\n",
    "### For 3.1 and 3.2, please produce the exact code (not pseudo-code):\n",
    "- 3.1 (15 points): Write a custom function that implements Experience Replay updates for\n",
    "an arbitrary minibatch size (use the TD(0) target as in the original DQN algorithm). The\n",
    "required input to this function must be listed as arguments and/or defined in the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.2 (20 points): Implement both, the Gym environment and an agent to learn to control a\n",
    "cart pole. The agent should be the PPO algorithm from the stable-baselines Python library.\n",
    "It should start from scratch, train for 50,000 steps, and be evaluated on 50 episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.3 (15 points): List at least 3 examples of RL agent components that you can import\n",
    "ready-to-use from the keras-rl Python library. For each component, provide both the exact\n",
    "method name (list of arguments is not necessary) and a description of what it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
