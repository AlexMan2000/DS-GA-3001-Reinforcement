{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (30 points)\n",
    "\n",
    "### Function Approximation in Reinforcement Learning:\n",
    "-  1.1 (5 points): What is the difference between tabular Q-learning and Q-learning with function approximation?\n",
    "- 1.2 (5 points): Why are TD updates by Gradient Descent called Semi-Gradient updates?\n",
    "- 1.3 (10 points): What challenges may tabular Q-learning meet when learning to play the game of Chess? Provide at least 2 answers.\n",
    "- 1.4 (10 points): What challenges may value function approximation meet in a Reinforcement Learning context compared to offline regression in a Supervised Learning context? Provide at least 3 answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: lightgreen;\" >\n",
    "\n",
    "- 1.1 **Tabular Q-learning** stores Q-values in a lookup table for every state-action pair, making it simple and effective for small, discrete environments but impractical for large or continuous spaces. **Q-learning with function approximatio** replaces the table with a parameterized function (typically a neural network) to estimate Q-values, allowing it to handle large or continuous spaces and generalize across unseen states, but at the cost of increased complexity and potential instability.\n",
    "- 1.2 The TD value function approximation target is $r_{t+1} + \\gamma v_{w}(s_{t+1})$ so typically the gradient of the squared TD error would be $\\nabla_{w}(r_{t+1}+\\gamma v_{w}(s_{t+1;w})-v_{w}(s_{t};w))$. But semi-gradient approach choose to use $\\nabla_{w}v_{w}(s_{t};w)$ as the gradient instead of both.\n",
    "- 1.3 Answer:\n",
    "    - Chess has an enormous number of possible board states so it is comutationally and memory-wise infeasible. \n",
    "    - The second reason is that each (s, a) pair is treated independently in the across the table. So even if it learns a good strategy in one situation, it cannot transfer that knowledge to similar board positions.\n",
    "- 1.4 Answer:\n",
    "    - In TD value function approximation, the target values depend on model's own predictions(e.g. bootstrapping) where the target value $v_{w}(s_{t+1};w) changes when $w$ is being updated. In contrast in supervised learning, the target value is typically fixed and won't change during training.\n",
    "    - The target values are often biased and correlated samples of the ground truth $v_{\\pi}(s_{t})$. In other words, training data used to parametrize the predictive function is gathered online and often non-stationary, which could result in local optimum or slow convergence.while in supervised learning setting the samples are usually assumed to be i.i.d.\n",
    "    - The environment in RL during exploration and exploitation may continuously evolve, or be too big for exhaustive sampling, and thus may never reach equilibrium whereas in supervised learning the environment is fixed, data is already collected and we don't have to explore.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise 2 (20 points)\n",
    "\n",
    "### Deep Q-Network (DQN):\n",
    "- 2.1 (10 points): What is Experience Replay? How does it benefit DQN?\n",
    "- 2.2 (10 points): Provide a complete pseudo-code for the DQN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: lightgreen;\" >\n",
    "\n",
    "- 2.1 Answer:\n",
    "    - Experience Replay is a method where the agent stores its past experiences (transitions) in a replay buffer: $(s, a, r, s^{'})$. During training, instead of learning from the most recent transition, the agent randomly samples a batch of past experiences from this buffer to update its Q-network.\n",
    "    - Random sampling fromthe replay buffer makes training data more i.i.d, the estimation of value target more stationary which could speed up convergence. Since each experience is used multiple times, they allow the agent the learn more effectively and thoroughly from the past experiences without new interactions with the environment all the time. \n",
    "- 2.2 The pseudocode is as follows:\n",
    "\n",
    "    Initialize:\n",
    "    - Q-network parameters $w$ arbitraraily\n",
    "    - Target Q-network parameters $w_{target} \\leftarrow w$\n",
    "    - Policy $\\pi_{s}$ (e.g. $\\epsilon$-greedy based on $q_{w}(s, a)$)\n",
    "    - Replay buffer $D \\leftarrow$ empty\n",
    "    - Hyperparameters: learning rate $\\alpha$, discount factor $\\gamma$, batch size $B$, target network update frequency $C$\n",
    "    - Step counter $t\\leftarrow 0$\n",
    "\n",
    "    Loop forever:\n",
    "\n",
    "    - Initialize environment and state $s$\n",
    "\n",
    "    - Loop until episode ends:\n",
    "        - Select action $a$ from $s$ using policy $\\pi_{s}$\n",
    "        - Execute $a$, obverse reward $r$ and next state $s'$\n",
    "        - Store the transition $(s, a, r, s')$ in the replay buffer $D$\n",
    "        - Update parameters $w$:\n",
    "            $w \\leftarrow w - \\alpha * (r + \\gamma * \\text{max}_{a'}q_{w}^{target}(s', a') - q_{w}(s, a))\\nabla_{w}q_{w}(s, a)$\n",
    "        - Update policy $\\pi_{s}$ based on $q_{w}(s, a)$ (e.g. with epsilon-greedy)\n",
    "        - Set $s \\leftarrow s'$\n",
    "        - Increment $t \\leftarrow t + 1$\n",
    "\n",
    "        - If $t \\mod C == 0$:\n",
    "            - Sample mini-batches from the replay buffer $D$\n",
    "            - For each $(s_{i}, a_{i}, r_{i}, s_{i+1})$ in $D$:\n",
    "                - Update target network: $w_{target} \\leftarrow w_{target} - \\alpha * (r_{i} + \\gamma * \\text{max}_{a'}q_{w}^{target}(s_{i+1}, a') - q_{w}(s_{i}, a_{i}))\\nabla_{w}q_{w}(s_{i}, a_{i})$\n",
    "            \n",
    "\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (50 points)\n",
    "### For 3.1 and 3.2, please produce the exact code (not pseudo-code):\n",
    "- 3.1 (15 points): Write a custom function that implements Experience Replay updates for\n",
    "an arbitrary minibatch size (use the TD(0) target as in the original DQN algorithm). The\n",
    "required input to this function must be listed as arguments and/or defined in the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def experience_replay(replay_buffer, mini_batch_size, model, target_model, gamma):\n",
    "    \"\"\"\n",
    "    Perform Experience Replay update using a minibatch of transitions.\n",
    "\n",
    "    Args:\n",
    "        replay_buffer (list): List of stored transitions (s, a, r, s', done),\n",
    "                              where each transition is a tuple.\n",
    "        mini_batch_size (int): Number of transitions to sample in each minibatch.\n",
    "        model (object): Q-network used to predict current Q-values (online network).\n",
    "        target_model (object): Target Q-network used to compute TD targets.\n",
    "        gamma (float): Discount factor (0 < gamma <= 1).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Do nothing if the buffer has too few samples\n",
    "    if len(replay_buffer) < mini_batch_size:\n",
    "        return\n",
    "\n",
    "    # Sample a random minibatch\n",
    "    minibatch = random.sample(replay_buffer, mini_batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    states = np.array(states)\n",
    "    next_states = np.array(next_states)\n",
    "\n",
    "    # Predict Q-values for current states and next states\n",
    "    q_values = model.predict(states, verbose=0)   \n",
    "    next_q_values = target_model.predict(next_states, verbose=0)\n",
    "\n",
    "    # Prepare target batch\n",
    "    target_batch = q_values.copy()\n",
    "\n",
    "    for i in range(mini_batch_size):\n",
    "        if dones[i]:\n",
    "            target = rewards[i]\n",
    "        else:\n",
    "            target = rewards[i] + gamma * np.max(next_q_values[i])\n",
    "\n",
    "        # Update only the action taken\n",
    "        target_batch[i][actions[i]] = target\n",
    "\n",
    "    # Perform one step of gradient descent on the batch\n",
    "    model.fit(states, target_batch, epochs=1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.2 (20 points): Implement both, the Gym environment and an agent to learn to control a\n",
    "cart pole. The agent should be the PPO algorithm from the stable-baselines Python library.\n",
    "It should start from scratch, train for 50,000 steps, and be evaluated on 50 episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.4     |\n",
      "|    ep_rew_mean     | 20.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 4372     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 23.6        |\n",
      "|    ep_rew_mean          | 23.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2006        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009306562 |\n",
      "|    clip_fraction        | 0.0906      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | -0.00134    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.73        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 42.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 32.7        |\n",
      "|    ep_rew_mean          | 32.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1696        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012012686 |\n",
      "|    clip_fraction        | 0.099       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 28.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 46.2         |\n",
      "|    ep_rew_mean          | 46.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1524         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073618335 |\n",
      "|    clip_fraction        | 0.065        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.637       |\n",
      "|    explained_variance   | 0.178        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.7         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0173      |\n",
      "|    value_loss           | 52.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 62.4        |\n",
      "|    ep_rew_mean          | 62.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1456        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008103361 |\n",
      "|    clip_fraction        | 0.0623      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.617      |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 60.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 81.5        |\n",
      "|    ep_rew_mean          | 81.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1413        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009026144 |\n",
      "|    clip_fraction        | 0.0798      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.604      |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.4        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 59.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 96.5         |\n",
      "|    ep_rew_mean          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1383         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059190076 |\n",
      "|    clip_fraction        | 0.0687       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.597       |\n",
      "|    explained_variance   | 0.627        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0126      |\n",
      "|    value_loss           | 52.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 112          |\n",
      "|    ep_rew_mean          | 112          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1361         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057499884 |\n",
      "|    clip_fraction        | 0.0509       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.574       |\n",
      "|    explained_variance   | 0.435        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.71         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00786     |\n",
      "|    value_loss           | 69           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 129          |\n",
      "|    ep_rew_mean          | 129          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1351         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042321132 |\n",
      "|    clip_fraction        | 0.0157       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.573       |\n",
      "|    explained_variance   | 0.491        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.6         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00452     |\n",
      "|    value_loss           | 57.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 148          |\n",
      "|    ep_rew_mean          | 148          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1347         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036846132 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.555       |\n",
      "|    explained_variance   | 0.0491       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.5         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00388     |\n",
      "|    value_loss           | 51.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 167         |\n",
      "|    ep_rew_mean          | 167         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1341        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002854201 |\n",
      "|    clip_fraction        | 0.00713     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.56        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.000455   |\n",
      "|    value_loss           | 27.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 185         |\n",
      "|    ep_rew_mean          | 185         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1335        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006573299 |\n",
      "|    clip_fraction        | 0.0854      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.34        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 13.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 204          |\n",
      "|    ep_rew_mean          | 204          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1327         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051791095 |\n",
      "|    clip_fraction        | 0.0507       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.552       |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.7         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00889     |\n",
      "|    value_loss           | 30           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 221          |\n",
      "|    ep_rew_mean          | 221          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1320         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007217011 |\n",
      "|    clip_fraction        | 0.0159       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.549       |\n",
      "|    explained_variance   | 0.312        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.04         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000753    |\n",
      "|    value_loss           | 27           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 238          |\n",
      "|    ep_rew_mean          | 238          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1298         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043828646 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.537       |\n",
      "|    explained_variance   | 0.749        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.109        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000663    |\n",
      "|    value_loss           | 2.97         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 256         |\n",
      "|    ep_rew_mean          | 256         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1300        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009187631 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.522      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.213       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 1.34        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 273          |\n",
      "|    ep_rew_mean          | 273          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1295         |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074016694 |\n",
      "|    clip_fraction        | 0.0754       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.526       |\n",
      "|    explained_variance   | 0.131        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0587       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00381     |\n",
      "|    value_loss           | 0.697        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 289           |\n",
      "|    ep_rew_mean          | 289           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1295          |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 28            |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016312081 |\n",
      "|    clip_fraction        | 0.000928      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.532        |\n",
      "|    explained_variance   | 0.158         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 12.4          |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.00099      |\n",
      "|    value_loss           | 25.7          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 310           |\n",
      "|    ep_rew_mean          | 310           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1298          |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 29            |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00076541386 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.537        |\n",
      "|    explained_variance   | 0.16          |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 41.9          |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.000595     |\n",
      "|    value_loss           | 26            |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 327          |\n",
      "|    ep_rew_mean          | 327          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1300         |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057115494 |\n",
      "|    clip_fraction        | 0.0408       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.522       |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.164        |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00595     |\n",
      "|    value_loss           | 1.12         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 342         |\n",
      "|    ep_rew_mean          | 342         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1300        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008689846 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.532      |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.4         |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00805    |\n",
      "|    value_loss           | 6.99        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 356          |\n",
      "|    ep_rew_mean          | 356          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1303         |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073712002 |\n",
      "|    clip_fraction        | 0.053        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.522       |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0351       |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00734     |\n",
      "|    value_loss           | 0.323        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 372          |\n",
      "|    ep_rew_mean          | 372          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1305         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055124117 |\n",
      "|    clip_fraction        | 0.0558       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.504       |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00287      |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00472     |\n",
      "|    value_loss           | 0.0377       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 387        |\n",
      "|    ep_rew_mean          | 387        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1304       |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 37         |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00320676 |\n",
      "|    clip_fraction        | 0.0153     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.495     |\n",
      "|    explained_variance   | -0.0404    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00686    |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.001     |\n",
      "|    value_loss           | 0.0236     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | 400         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1304        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003916726 |\n",
      "|    clip_fraction        | 0.0379      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.517      |\n",
      "|    explained_variance   | -0.0293     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00551     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00153    |\n",
      "|    value_loss           | 0.0161      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Study_Notes_Backup\\Data_Science\\DS-GA-3001-Reinforcement\\.venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "f:\\Study_Notes_Backup\\Data_Science\\DS-GA-3001-Reinforcement\\.venv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 500.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from pyglet.window import key \n",
    "import tensorflow as tf\n",
    "from keras import __version__\n",
    "tf.keras.__version__ = __version__\n",
    "from stable_baselines3 import PPO \n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "# Answer begins here:\n",
    "def create_cartpole_env():\n",
    "    env_cartpole = gym.make(\"CartPole-v1\")   \n",
    "    env_cartpole.reset()\n",
    "\n",
    "    return env_cartpole\n",
    "\n",
    "\n",
    "def create_agent(env):\n",
    "    \"\"\"\n",
    "    This model will map the state to a q-value for each action.\n",
    "    Args:\n",
    "        env (gym.Env): The environment to use for the agent.\n",
    "    \"\"\"\n",
    "    dqn_agent = PPO(\"MlpPolicy\", env, verbose=1) # Mlp for continuous low-dimensional input\n",
    "    return dqn_agent\n",
    "\n",
    "\n",
    "def train_agent(agent, nb_steps=50000):\n",
    "    \"\"\"\n",
    "    Train the agent for a given number of steps.\n",
    "    Args:\n",
    "        agent (stable_baselines3.PPO): The agent to train.\n",
    "        nb_steps (int): The number of steps to train the agent.\n",
    "    \"\"\"\n",
    "    agent.learn(total_timesteps=nb_steps)\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, env, nb_episodes=50):\n",
    "    \"\"\"\n",
    "    Evaluate the agent for a given number of episodes.\n",
    "    Args:\n",
    "        agent (stable_baselines3.PPO): The agent to evaluate.\n",
    "        env (gym.Env): The environment to use for the evaluation.\n",
    "        nb_episodes (int): The number of episodes to evaluate the agent.\n",
    "    \"\"\"\n",
    "    mean_reward, std_reward = evaluate_policy(agent, env, n_eval_episodes=nb_episodes)\n",
    "    print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Triggers\n",
    "env_cartpole = create_cartpole_env()\n",
    "agent = create_agent(env_cartpole)\n",
    "train_agent(agent, nb_steps=50000)\n",
    "evaluate_agent(agent, env_cartpole, nb_episodes=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.3 (15 points): List at least 3 examples of RL agent components that you can import\n",
    "ready-to-use from the keras-rl Python library. For each component, provide both the exact\n",
    "method name (list of arguments is not necessary) and a description of what it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: lightgreen;\" >\n",
    "\n",
    "- `SequentialMemory()` in the `rl.memory`: This is a replay buffer used to store the agent's past experiences in the form of transitions. It enables experience replay by randomly sampling past experiences to break correlation and improve sample efficiency.\n",
    "- `DQNAgent()` in the `rl.agent`: This is the main Deep Q-Learning Agent class.\n",
    "It combines a neural network model, memory buffer, and policy to learn Q-values and interact with the environment using the DQN algorithm.\n",
    "- `LinearAnnealedPolicy()` in the `rl.policy`: It is a policy wrapper that gradually reduces a parameter (typically ε in ε-greedy) linearly over time to balance exploration and exploitation during training.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\Study_Notes_Backup\\Data_Science\\DS-GA-3001-Reinforcement\\.venv\\lib\\site-packages\\rl\\agents\\ddpg.py:9: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
