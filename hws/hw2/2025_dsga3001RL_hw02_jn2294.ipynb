{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 (20 points)\n",
    "Markov Decision Processes and Bellman Equations:\n",
    "- 1.1 (10 points): What is a Markov Decision Process, and what is the Markov property?\n",
    "- 1.2 (5 points): Write $v_{π}(s)$ as a function of $q_{π}(s,a)$\n",
    "- 1.3 (5 points): Write $v_{∗}(s)$ as a function of $q_{∗}(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: lightgreen;\" >\n",
    "\n",
    "- 1.1 From lecture notes, the MDP is a mathematical idealization of sequential goal-directed learning from interaction with an environment.\n",
    "- 1.2 $v_{\\pi}(s) = \\mathbb{E}_{a\\in \\mathcal{A}}[q_{\\pi}(s,a)] = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)q_{\\pi}(s,a)$\n",
    "- 1.3 $v_{∗}(s) = \\max_{a\\in \\mathcal{A}} q_{∗}(s,a)$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 (30 points)\n",
    "Dynamic Programming:\n",
    "- 2.1 (20 points): In the gridworld problem below, the goal is to reach state g, the reward is -1 for moving to any state except state g where it is 0, actions in each state are up, down, right or left (by 1 step), and actions taking the agent off the grid leaves the state unchanged. \n",
    "\n",
    "<img style=\"display: block; margin: 0 auto\" src=\"./prob2.1.png\" width=\"400\">\n",
    "\n",
    "What are the final state values after convergence of the Value Iteration algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.2 (5 points): What is the key difference between the Policy Iteration algorithm and the Value Iteration algorithm?\n",
    "\n",
    "<span style=\"color: lightgreen;\">\n",
    "\n",
    "- Policy iteration contains two phases: policy evaluation and policy improvement. Policy evaluation starts with a random policy and compute the value function of all states based on that policy. Policy improvement updates the policy based on the value function. It aims to pick the maximum value action in the second phase and is guaranteed to improve the policy w.r.t expected reward. The algorithm stops until policy stablizes.\n",
    "\n",
    "- Value iteration starts with random state values. Itc only onsists of a single phase, which compute value and update policy at the same time but only aims to maximize the value function at each update. It stops until the state values stablize.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.3 (5 points): What is the key difference between synchronous Value Iteration and asynchronous Value Iteration?\n",
    "\n",
    "<span style=\"color: lightgreen;\">\n",
    "\n",
    "- Synchronous value iteration updates all the state values during each iteration of DP. It requires more iterations to wait for all states to converge(even if some states are already optimal or we don't care about their values). \n",
    "\n",
    "- Asynchronous value iteration updates the state values one at a time, rather than waiting for a full pass. Generally it converges faster than synchronous value iteration. But sometimes a poorly chosen update order can lead to slow convergence or even divergence. Also it can be more memory efficient.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 (25 points)\n",
    "Monte Carlo and Temporal Difference:\n",
    "\n",
    "- 3.1 (5 points): What is the key difference between Dynamic Programming and sample-based Reinforcement Learning?\n",
    "\n",
    "<span style=\"color: lightgreen;\">\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.2 (5 points): List at least three differences between Monte-Carlo policy evaluation and Temporal Difference policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.3 (10 points): Why is Q-learning considered an off-policy control method? Provide an example where using an off-policy method is more appropriate than using an on-policy method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.4 (5 points): Why is Q-learning overly optimistic? And how does Double Q-learning help address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4 (25 points)\n",
    "Produce the code to do the following:\n",
    "- 4.1 (10 points): Write a function that implements the ε-greedy action-selection strategy. It should return the action selected. The required input to this function should be listed as arguments and/or defined in the function’s docstring.\n",
    "- 4.2 (15 points): Write a function that implements a q-value update for tabular Q-learning, given a reward r observed after sampling an action a in a state s. It should return the updated q-value. The required input to this function should be listed as arguments and/or defined in\n",
    "the function’s docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
