{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 (20 points)\n",
    "Markov Decision Processes and Bellman Equations:\n",
    "- 1.1 (10 points): What is a Markov Decision Process, and what is the Markov property?\n",
    "- 1.2 (5 points): Write $v_{π}(s)$ as a function of $q_{π}(s,a)$\n",
    "- 1.3 (5 points): Write $v_{∗}(s)$ as a function of $q_{∗}(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: lightgreen;\" >\n",
    "\n",
    "- 1.1 From lecture notes, the MDP is a mathematical idealization of sequential goal-directed learning from interaction with an environment.\n",
    "- 1.2 $v_{\\pi}(s) = \\mathbb{E}_{a\\in \\mathcal{A}}[q_{\\pi}(s,a)] = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)q_{\\pi}(s,a)$\n",
    "- 1.3 $v_{∗}(s) = \\max_{a\\in \\mathcal{A}} q_{∗}(s,a)$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 (30 points)\n",
    "Dynamic Programming:\n",
    "- 2.1 (20 points): In the gridworld problem below, the goal is to reach state g, the reward is -1 for moving to any state except state g where it is 0, actions in each state are up, down, right or left (by 1 step), and actions taking the agent off the grid leaves the state unchanged. \n",
    "\n",
    "<img style=\"display: block; margin: 0 auto\" src=\"./prob2.1.png\" width=\"400\">\n",
    "\n",
    "What are the final state values after convergence of the Value Iteration algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: lightgreen;\">\n",
    "Assume that the state of interests have the following indices:\n",
    "</span>\n",
    "\n",
    "<img style=\"display: block; margin: 0 auto\" src=\"./prob2.1_sol.png\" width=\"400\">\n",
    "\n",
    "<span style=\"color: lightgreen;\">\n",
    "Then during the value iteration process, the optimal state values would be to minimize the total negative reward, which is the negative of Manhattan distance to state g. So the optimal state values would be: </span>\n",
    "\n",
    "<img style=\"display: block; margin: 0 auto\" src=\"./prob2.1_sol_values.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.2 (5 points): What is the key difference between the Policy Iteration algorithm and the Value Iteration algorithm?\n",
    "\n",
    "<span style=\"color: lightgreen;\">\n",
    "\n",
    "- Policy iteration contains two phases: policy evaluation and policy improvement. Policy evaluation starts with a random policy and compute the value function of all states based on that policy. Policy improvement updates the policy based on the value function. It aims to pick the maximum value action in the second phase and is guaranteed to improve the policy w.r.t expected reward. The algorithm stops until policy stablizes.\n",
    "\n",
    "- Value iteration starts with random state values. Itc only onsists of a single phase, which compute value and update policy at the same time but only aims to maximize the value function at each update. It stops until the state values stablize.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.3 (5 points): What is the key difference between synchronous Value Iteration and asynchronous Value Iteration?\n",
    "\n",
    "<span style=\"color: lightgreen;\">\n",
    "\n",
    "- Synchronous value iteration updates all the state values during each iteration of DP. It requires more iterations to wait for all states to converge(even if some states are already optimal or we don't care about their values). \n",
    "\n",
    "- Asynchronous value iteration updates the state values one at a time, rather than waiting for a full pass. Generally it converges faster than synchronous value iteration. But sometimes a poorly chosen update order can lead to slow convergence or even divergence. Also it can be more memory efficient.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 (25 points)\n",
    "Monte Carlo and Temporal Difference:\n",
    "\n",
    "- 3.1 (5 points): What is the key difference between Dynamic Programming and sample-based Reinforcement Learning?\n",
    "\n",
    "<span style=\"color: lightgreen;\">\n",
    "\n",
    "- Dynamic Programming is a model-based approach that uses the transition and reward functions to compute the value function. It requires a complete model of the environment. \n",
    "\n",
    "- Sample-based Reinforcement Learning is a model-free approach that learns the value function from experience(trial and error) by sampling the state-action space. It does not require a complete model of the environment.\n",
    "\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.2 (5 points): List at least three differences between Monte-Carlo policy evaluation and Temporal Difference policy evaluation\n",
    "\n",
    "<span style=\"color: lightgreen;\">\n",
    "\n",
    "<ul>\n",
    "<b>Monte-Carlo policy evaluation:</b>\n",
    "<li>Uses the entire episode to update the value function. </li>\n",
    "<li>It doesn't need any bootstrapping process(no need to estimate the value of the next state). </li>\n",
    "<li>It only works in episodic (terminating) environments. </li>\n",
    "<li>It is unbiased but has high variance. </li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<b>Temporal Difference policy evaluation:</b>\n",
    "\n",
    "<li>Doesn't need complete episodes and it can use bootstrapping process(estimate the value of the next state). </li>\n",
    "<li>It works in continuing(non-terminating) environments. </li>\n",
    "<li>It is biased but has lower variance. </li>\n",
    "</ul>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.3 (10 points): Why is Q-learning considered an off-policy control method? Provide an example where using an off-policy method is more appropriate than using an on-policy method.\n",
    "\n",
    "<span style=\"color: lightgreen;\">\n",
    "Because it learns the optimal policy while following a potentially different behavior policy. More specifically:\n",
    "<ul>\n",
    "<li>Q-learning systematically updates the greedy policy whateverthe behavior policy followed is, thus keeping learning focusedon greedy actions even when the agent explores other action</li>\n",
    "<li>It can learn the optimal policy even when the behavior policy is not optimal</li>\n",
    "</ul>\n",
    "\n",
    "In the field of autonomous driving field, if the car wants to optimize its driving policy, by observing human's driving behaviors(their own action space), but it can't follow the human drivers' action space exactly. So using an off-policy method is more appropriate than using an on-policy method. It is better than on-policy method because:\n",
    "<ul>\n",
    "<li>Human drivers may not always take the optimal action, if the car follows the human drivers' action space exactly, it may learn a sub-optimal policy.</li>\n",
    "<li>With off-policy Q-learning, the car observes human actions but learns the best possible driving policy by always selecting the optimal Q-values (max operator).</li>\n",
    "</ul>\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.4 (5 points): Why is Q-learning overly optimistic? And how does Double Q-learning help address this issue?\n",
    "\n",
    "<span style=\"color: lightgreen;\">\n",
    "Since the update rule of off-policy Q-learning is: \n",
    "\n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$\n",
    "\n",
    "But in practice, $Q(s', a')$ may contain errors due to noise, limited data, or early-stage learning. Thus the max operator may overselect the overestimated Q-values, leading to overly optimistic behavior.\n",
    "\n",
    "Double Q-learning helps address this issue by using two Q-functions that separates the action selection ($Q_1$) and value estimation ($Q_2$) processes. Basically it uses a separate q function to evaluate the maximum q-action based on limited experiences. It acts as a referee to judge whether the max operator acutally select a true good action, which will be reflected in the $Q_2(s, a)$'s value.\n",
    "\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4 (25 points)\n",
    "Produce the code to do the following:\n",
    "- 4.1 (10 points): Write a function that implements the ε-greedy action-selection strategy. It should return the action selected. The required input to this function should be listed as arguments and/or defined in the function’s docstring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def epsilon_greedy(q_values, epsilon):\n",
    "    \"\"\"\n",
    "    Implement the ε-greedy action-selection strategy.\n",
    "\n",
    "    Args:\n",
    "        q_values (array-like): The Q-values for each action in the current state.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the selected action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly select an action with probability epsilon\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(len(q_values))\n",
    "    # Greedy policy with probabilty 1 - epsilon\n",
    "    else:\n",
    "        return np.argmax(q_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4.2 (15 points): Write a function that implements a q-value update for tabular Q-learning, given a reward r observed after sampling an action a in a state s. It should return the updated q-value. The required input to this function should be listed as arguments and/or defined in\n",
    "the function’s docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_update(q_values, state, action, reward, alpha, gamma, next_state):\n",
    "    \"\"\"\n",
    "    Update the Q-value for a given state-action pair.\n",
    "\n",
    "    Args:\n",
    "        q_values (array-like): The Q-values for each action in the current state.\n",
    "        state (int): The current state.\n",
    "        action (int): The action taken.\n",
    "        reward (float): The reward received.\n",
    "        alpha (float): The learning rate.\n",
    "        gamma (float): The discount factor.\n",
    "        next_state (int): The next state.\n",
    "\n",
    "    Returns:    \n",
    "        array-like: The updated Q-values.   \n",
    "    \"\"\"\n",
    "    # Get the maximum Q-value for the next state\n",
    "    max_next_q = np.max(q_values[next_state])\n",
    "    \n",
    "    # Update the Q-value for the current state-action pair\n",
    "    q_values[state, action] += alpha * (reward + gamma * max_next_q - q_values[state, action])\n",
    "    return q_values\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
